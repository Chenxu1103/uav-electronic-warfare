#!/usr/bin/env python3
"""
é«˜çº§ä¼˜åŒ–æµ‹è¯•ç³»ç»Ÿ - å®ç°çœŸå®æœ‰æ•ˆçš„æ€§èƒ½æ”¹è¿›

é›†æˆå¢å¼ºå¹²æ‰°ç³»ç»Ÿï¼Œç¡®ä¿è·å¾—çœŸå®çš„å¹²æ‰°ç‡ã€æˆåŠŸç‡ç­‰å…³é”®æŒ‡æ ‡ã€‚
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from src.environment.electronic_warfare_env import ElectronicWarfareEnv
from src.algorithms.ad_ppo import ADPPO
from intelligent_reward_designer import IntelligentRewardDesigner, RewardShapeOptimizer
from enhanced_jamming_system import EnhancedJammingSystem, RealTimePerformanceCalculator

class AdvancedOptimizationTester:
    """é«˜çº§ä¼˜åŒ–æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.reward_designer = IntelligentRewardDesigner()
        self.reward_shaper = RewardShapeOptimizer()
        self.jamming_system = EnhancedJammingSystem()
        self.performance_calculator = RealTimePerformanceCalculator()
        
        # åˆ›å»ºä¼˜åŒ–çš„ç¯å¢ƒ
        self.env = self._create_enhanced_environment()
        
        # æ€§èƒ½å†å²è®°å½•
        self.performance_history = []
        
    def _create_enhanced_environment(self):
        """åˆ›å»ºå¢å¼ºçš„ç¯å¢ƒ"""
        env = ElectronicWarfareEnv(
            num_uavs=3,
            num_radars=2,
            env_size=1500.0,
            max_steps=150
        )
        
        # ä¼˜åŒ–å¥–åŠ±æƒé‡ - å¼ºåŒ–å¹²æ‰°å¯¼å‘
        env.reward_weights.update({
            'jamming_success': 200.0,           # å¤§å¹…å¢åŠ å¹²æ‰°æˆåŠŸå¥–åŠ±
            'partial_success': 100.0,           # éƒ¨åˆ†æˆåŠŸå¥–åŠ±
            'jamming_attempt_reward': 50.0,     # å¢åŠ å°è¯•å¹²æ‰°å¥–åŠ±
            'approach_reward': 30.0,            # å¢åŠ æ¥è¿‘å¥–åŠ±
            'coordination_reward': 80.0,        # å¢åŠ åè°ƒå¥–åŠ±
            'goal_reward': 500.0,               # ç›®æ ‡å®Œæˆå¥–åŠ±
            'distance_penalty': -0.00001,       # å‡å°è·ç¦»æƒ©ç½š
            'energy_penalty': -0.001,           # å‡å°èƒ½é‡æƒ©ç½š
            'detection_penalty': -0.05,         # å‡å°æ£€æµ‹æƒ©ç½š
            'death_penalty': -50.0,             # å‡å°æ­»äº¡æƒ©ç½š
            'stealth_reward': 10.0,             # éšèº«å¥–åŠ±
            'reward_scale': 1.0,                # å¥–åŠ±ç¼©æ”¾
            'min_reward': -20.0,                # æœ€å°å¥–åŠ±
            'max_reward': 300.0,                # æœ€å¤§å¥–åŠ±
        })
        
        return env
    
    def run_advanced_optimization_test(self, episodes=120):
        """è¿è¡Œé«˜çº§ä¼˜åŒ–æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹é«˜çº§ä¼˜åŒ–æµ‹è¯•...")
        print(f"ç›®æ ‡: åœ¨{episodes}ä¸ªå›åˆå†…å®ç°çœŸå®æ€§èƒ½æ”¹è¿›")
        
        # åˆ›å»ºä¼˜åŒ–çš„AD-PPOæ™ºèƒ½ä½“
        agent = self._create_enhanced_agent()
        
        # ä¼˜åŒ–å‰çš„åŸºçº¿æµ‹è¯•
        print("\nğŸ“Š åŸºçº¿æ€§èƒ½æµ‹è¯•...")
        baseline_metrics = self._evaluate_comprehensive_performance(agent, 15, "åŸºçº¿")
        
        # å¼€å§‹ä¼˜åŒ–è®­ç»ƒ
        print(f"\nğŸ¯ å¼€å§‹{episodes}å›åˆé«˜çº§ä¼˜åŒ–è®­ç»ƒ...")
        optimized_metrics = self._run_enhanced_training(agent, episodes)
        
        # æœ€ç»ˆæ€§èƒ½æµ‹è¯•
        print("\nğŸ“Š ä¼˜åŒ–åæ€§èƒ½æµ‹è¯•...")
        final_metrics = self._evaluate_comprehensive_performance(agent, 15, "ä¼˜åŒ–å")
        
        # ç”Ÿæˆè¯¦ç»†å¯¹æ¯”æŠ¥å‘Š
        self._generate_advanced_report(baseline_metrics, final_metrics, optimized_metrics)
        
        return baseline_metrics, final_metrics, optimized_metrics
    
    def _create_enhanced_agent(self):
        """åˆ›å»ºå¢å¼ºçš„AD-PPOæ™ºèƒ½ä½“"""
        state_dim = self.env.observation_space.shape[0]
        action_dim = self.env.action_space.shape[0]
        
        agent = ADPPO(
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_dim=256,      # å¢åŠ ç½‘ç»œå®¹é‡
            lr=3e-4,             # ä¼˜åŒ–å­¦ä¹ ç‡
            gamma=0.99,          # è¾ƒé«˜æŠ˜æ‰£å› å­
            gae_lambda=0.95,
            clip_param=0.2,
            value_loss_coef=0.5,
            entropy_coef=0.015,  # å¢åŠ æ¢ç´¢
            max_grad_norm=0.5,
            device='cpu'
        )
        
        return agent
    
    def _evaluate_comprehensive_performance(self, agent, num_episodes, phase_name):
        """è¯„ä¼°ç»¼åˆæ€§èƒ½"""
        print(f"  è¯„ä¼°{phase_name}æ€§èƒ½ ({num_episodes}å›åˆ)...")
        
        all_metrics = []
        
        for episode in range(num_episodes):
            episode_data = self._run_comprehensive_episode(agent, evaluation=True)
            
            # ä½¿ç”¨å¢å¼ºæ€§èƒ½è®¡ç®—å™¨
            metrics = self.performance_calculator.calculate_comprehensive_metrics(
                self.env, episode_data
            )
            
            all_metrics.append(metrics)
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        for key in all_metrics[0].keys():
            values = [m[key] for m in all_metrics]
            avg_metrics[key] = np.mean(values)
        
        # æ‰“å°å…³é”®æŒ‡æ ‡
        print(f"    å¹³å‡å¥–åŠ±: {avg_metrics['average_reward']:.2f}")
        print(f"    æˆåŠŸç‡: {avg_metrics['success_rate']:.2%}")
        print(f"    å¹²æ‰°ç‡: {avg_metrics['jamming_ratio']:.2%}")
        print(f"    ä¾¦å¯Ÿå®Œæˆåº¦: {avg_metrics['reconnaissance_completion']:.3f}")
        print(f"    å®‰å…¨åŒºåŸŸæ—¶é—´: {avg_metrics['safe_zone_development_time']:.2f}")
        print(f"    ä¾¦å¯Ÿåä½œç‡: {avg_metrics['reconnaissance_cooperation_rate']:.1f}%")
        print(f"    å¹²æ‰°åä½œç‡: {avg_metrics['jamming_cooperation_rate']:.1f}%")
        
        return avg_metrics
    
    def _run_comprehensive_episode(self, agent, evaluation=False):
        """è¿è¡Œç»¼åˆè¯„ä¼°å›åˆ"""
        state = self.env.reset()
        total_reward = 0
        step = 0
        
        # è®°å½•è¯¦ç»†è½¨è¿¹
        uav_trajectory = []
        jamming_attempts = 0
        successful_jams = 0
        
        while step < self.env.max_steps:
            # é€‰æ‹©åŠ¨ä½œ
            if evaluation:
                action, _, _ = agent.select_action(state, deterministic=True)
            else:
                action, log_prob, value = agent.select_action(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = self.env.step(action)
            
            # è®°å½•è½¨è¿¹
            uav_positions = [uav.position.copy() for uav in self.env.uavs if uav.is_alive]
            uav_trajectory.append(uav_positions)
            
            # å®æ—¶å¹²æ‰°è¯„ä¼°
            radar_positions = [radar.position for radar in self.env.radars]
            jamming_results = self.jamming_system.evaluate_cooperative_jamming(
                uav_positions, radar_positions
            )
            
            # æ›´æ–°ç¯å¢ƒä¸­çš„é›·è¾¾çŠ¶æ€
            for radar_idx, radar in enumerate(self.env.radars):
                if radar_idx < len(jamming_results['jamming_details']):
                    jamming_data = jamming_results['jamming_details'][radar_idx]
                    radar.is_jammed = jamming_data['is_jammed']
                    if jamming_data['is_jammed']:
                        successful_jams += 1
            
            jamming_attempts += len(uav_positions)
            
            # è®­ç»ƒæ¨¡å¼ä¸‹å­˜å‚¨ç»éªŒ
            if not evaluation:
                agent.buffer.add(
                    state=state,
                    action=action,
                    reward=reward,
                    next_state=next_state,
                    done=done,
                    log_prob=log_prob,
                    value=value
                )
            
            state = next_state
            total_reward += reward
            step += 1
            
            if done:
                break
        
        # è®­ç»ƒæ¨¡å¼ä¸‹æ›´æ–°æ¨¡å‹
        if not evaluation and len(agent.buffer.states) > 0:
            _, _, last_value = agent.select_action(state)
            agent.buffer.compute_returns_and_advantages(last_value, agent.gamma, agent.gae_lambda)
            rollout = agent.buffer.get()
            agent.update(rollout)
            agent.buffer.clear()
        
        return {
            'total_reward': total_reward,
            'steps': step,
            'uav_trajectory': uav_trajectory,
            'jamming_attempts': jamming_attempts,
            'successful_jams': successful_jams
        }
    
    def _run_enhanced_training(self, agent, episodes):
        """è¿è¡Œå¢å¼ºè®­ç»ƒ"""
        training_metrics = []
        
        for episode in range(episodes):
            # æ¯15å›åˆè¯„ä¼°ä¸€æ¬¡æ€§èƒ½å¹¶è°ƒæ•´å¥–åŠ±
            if episode % 15 == 0 and episode > 0:
                current_metrics = self._evaluate_comprehensive_performance(
                    agent, 8, f"ç¬¬{episode}å›åˆ"
                )
                
                # ä½¿ç”¨æ™ºèƒ½å¥–åŠ±è®¾è®¡å™¨è°ƒæ•´ç¯å¢ƒ
                new_weights = self.reward_designer.design_adaptive_rewards(
                    self.env, current_metrics, episode
                )
                self.env.reward_weights.update(new_weights)
                
                # è®°å½•æ€§èƒ½
                training_metrics.append({
                    'episode': episode,
                    'metrics': current_metrics
                })
                
                self.performance_history.append(current_metrics)
            
            # è®­ç»ƒä¸€ä¸ªå›åˆ
            self._run_comprehensive_episode(agent, evaluation=False)
            
            # æ‰“å°è¿›åº¦
            if episode % 24 == 0:
                print(f"  è®­ç»ƒè¿›åº¦: {episode}/{episodes} ({episode/episodes*100:.1f}%)")
        
        return training_metrics
    
    def _generate_advanced_report(self, baseline, final, training_history):
        """ç”Ÿæˆé«˜çº§å¯¹æ¯”æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“ˆ é«˜çº§ä¼˜åŒ–æ•ˆæœæŠ¥å‘Š")
        print("="*80)
        
        # è®¡ç®—æ”¹è¿›å¹…åº¦
        improvements = {}
        for key in baseline:
            if key in final:
                baseline_val = baseline[key]
                final_val = final[key]
                
                if baseline_val != 0:
                    improvement = (final_val - baseline_val) / abs(baseline_val) * 100
                else:
                    improvement = final_val * 100
                
                improvements[key] = improvement
        
        # è¯¦ç»†æŒ‡æ ‡å¯¹æ¯”
        print(f"{'æŒ‡æ ‡':<30} {'åŸºçº¿å€¼':<15} {'ä¼˜åŒ–å':<15} {'æ”¹è¿›å¹…åº¦':<15} {'è®ºæ–‡ç›®æ ‡':<15}")
        print("-" * 90)
        
        metrics_info = {
            'average_reward': ('å¹³å‡å¥–åŠ±', 800.0),
            'success_rate': ('æˆåŠŸç‡ (%)', 60.0),
            'jamming_ratio': ('å¹²æ‰°ç‡ (%)', 70.0),
            'reconnaissance_completion': ('ä¾¦å¯Ÿå®Œæˆåº¦', 0.97),
            'safe_zone_development_time': ('å®‰å…¨åŒºåŸŸæ—¶é—´', 2.1),
            'reconnaissance_cooperation_rate': ('ä¾¦å¯Ÿåä½œç‡ (%)', 37.0),
            'jamming_cooperation_rate': ('å¹²æ‰°åä½œç‡ (%)', 34.0),
            'jamming_failure_rate': ('å¹²æ‰°å¤±æ•ˆç‡ (%)', 23.3),
            'overall_effectiveness': ('æ•´ä½“æœ‰æ•ˆæ€§', 0.8)
        }
        
        for key, (display_name, target) in metrics_info.items():
            if key in baseline and key in final:
                baseline_val = baseline[key]
                final_val = final[key]
                improvement = improvements.get(key, 0)
                
                if 'rate' in key or 'ratio' in key or key == 'success_rate':
                    if key == 'jamming_failure_rate':
                        # å¤±æ•ˆç‡è¶Šä½è¶Šå¥½
                        status = "âœ…" if final_val <= target else "âŒ"
                    else:
                        status = "âœ…" if final_val >= target/100 else "âŒ"
                    print(f"{display_name:<30} {baseline_val:.1%} {'':>6} {final_val:.1%} {'':>6} {improvement:+.1f}% {'':>8} {target:.1f}% {status}")
                else:
                    status = "âœ…" if final_val >= target else "âŒ"
                    print(f"{display_name:<30} {baseline_val:.3f} {'':>8} {final_val:.3f} {'':>8} {improvement:+.1f}% {'':>8} {target:.1f} {status}")
        
        print("\n" + "="*80)
        
        # å…³é”®æˆå°±
        print("ğŸ† å…³é”®æˆå°±:")
        achievements = []
        
        if final['jamming_ratio'] > 0.3:
            achievements.append(f"â€¢ å®ç° {final['jamming_ratio']:.1%} å¹²æ‰°ç‡ (åŸºçº¿: {baseline['jamming_ratio']:.1%})")
        
        if final['success_rate'] > 0.2:
            achievements.append(f"â€¢ è¾¾åˆ° {final['success_rate']:.1%} æˆåŠŸç‡ (åŸºçº¿: {baseline['success_rate']:.1%})")
        
        if final['safe_zone_development_time'] > 0.5:
            achievements.append(f"â€¢ å»ºç«‹ {final['safe_zone_development_time']:.1f}s å®‰å…¨åŒºåŸŸ (åŸºçº¿: {baseline['safe_zone_development_time']:.1f}s)")
        
        if final['jamming_cooperation_rate'] > baseline['jamming_cooperation_rate']:
            achievements.append(f"â€¢ å¹²æ‰°åä½œç‡æå‡è‡³ {final['jamming_cooperation_rate']:.1f}%")
        
        avg_reward_improvement = improvements.get('average_reward', 0)
        if avg_reward_improvement > 50:
            achievements.append(f"â€¢ å¹³å‡å¥–åŠ±æå‡ {avg_reward_improvement:.1f}%")
        
        for achievement in achievements:
            print(achievement)
        
        if not achievements:
            print("â€¢ å°šæœªå®ç°æ˜¾è‘—çªç ´ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        # è·ç¦»è®ºæ–‡ç›®æ ‡çš„å·®è·åˆ†æ
        print("\nğŸ“Š ä¸è®ºæ–‡ç›®æ ‡å·®è·åˆ†æ:")
        print("-" * 50)
        
        paper_targets = {
            'reconnaissance_completion': 0.97,
            'safe_zone_development_time': 2.1,
            'reconnaissance_cooperation_rate': 37.0,
            'jamming_cooperation_rate': 34.0,
            'jamming_failure_rate': 23.3
        }
        
        total_gap = 0
        target_count = 0
        
        for key, target in paper_targets.items():
            if key in final:
                current = final[key]
                if key == 'jamming_failure_rate':
                    gap = max(0, current - target) / target * 100
                elif key in ['reconnaissance_cooperation_rate', 'jamming_cooperation_rate']:
                    gap = abs(current - target) / target * 100
                else:
                    gap = abs(current - target) / target * 100
                
                total_gap += gap
                target_count += 1
                
                status = "âœ…" if gap < 30 else "âš ï¸" if gap < 60 else "âŒ"
                print(f"  {key}: å·®è· {gap:.1f}% {status}")
        
        avg_gap = total_gap / max(1, target_count)
        print(f"\nå¹³å‡å·®è·: {avg_gap:.1f}%")
        
        if avg_gap < 25:
            print("ğŸ‰ éå¸¸æ¥è¿‘è®ºæ–‡ç›®æ ‡ï¼")
        elif avg_gap < 50:
            print("ğŸ‘ è¾ƒå¥½åœ°æ¥è¿‘è®ºæ–‡ç›®æ ‡")
        else:
            print("ğŸ”§ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ä»¥æ¥è¿‘è®ºæ–‡ç›®æ ‡")
        
        # ä¼˜åŒ–å»ºè®®
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥ä¼˜åŒ–å»ºè®®:")
        self._generate_specific_recommendations(final, paper_targets)
        
        # ä¿å­˜ç»“æœ
        self._save_advanced_results(baseline, final, improvements, training_history)
    
    def _generate_specific_recommendations(self, current_metrics, targets):
        """ç”Ÿæˆå…·ä½“ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if current_metrics['jamming_ratio'] < 0.5:
            recommendations.append("â€¢ å¢å¼ºå¹²æ‰°æœºåˆ¶: æé«˜UAVå¹²æ‰°åŠŸç‡ï¼Œä¼˜åŒ–å¹²æ‰°è·ç¦»è®¡ç®—")
        
        if current_metrics['success_rate'] < 0.4:
            recommendations.append("â€¢ æ”¹è¿›ä»»åŠ¡æˆåŠŸåˆ¤å®š: é™ä½æˆåŠŸé˜ˆå€¼ï¼Œå¢åŠ æ¸è¿›å¼å¥–åŠ±")
        
        if current_metrics['safe_zone_development_time'] < 1.0:
            recommendations.append("â€¢ å¼ºåŒ–å®‰å…¨åŒºåŸŸå»ºç«‹: å¢åŠ æŒç»­å¹²æ‰°å¥–åŠ±ï¼Œä¼˜åŒ–å¤šUAVåä½œ")
        
        if current_metrics['jamming_cooperation_rate'] < 25:
            recommendations.append("â€¢ æå‡åä½œæ•ˆç‡: å®ç°æ™ºèƒ½ä»»åŠ¡åˆ†é…ï¼Œä¼˜åŒ–UAVé—´é€šä¿¡")
        
        if current_metrics['reconnaissance_cooperation_rate'] < 30:
            recommendations.append("â€¢ ä¼˜åŒ–ä¾¦å¯Ÿåä½œ: æ”¹è¿›ç¼–é˜Ÿæ§åˆ¶ï¼Œå¢åŠ åä½œæ¢ç´¢å¥–åŠ±")
        
        if not recommendations:
            recommendations.append("â€¢ ç»§ç»­ç²¾ç»†è°ƒä¼˜ç°æœ‰æœºåˆ¶ï¼Œä¿æŒæ€§èƒ½ç¨³å®šæ€§")
        
        for rec in recommendations:
            print(rec)
    
    def _save_advanced_results(self, baseline, final, improvements, training_history):
        """ä¿å­˜é«˜çº§ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = f"experiments/advanced_optimization/{timestamp}"
        os.makedirs(save_dir, exist_ok=True)
        
        # è½¬æ¢numpyç±»å‹
        def convert_numpy_types(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.integer, np.floating)):
                return obj.item()
            elif isinstance(obj, dict):
                return {k: convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            else:
                return obj
        
        results_data = {
            'baseline': convert_numpy_types(baseline),
            'final': convert_numpy_types(final),
            'improvements': convert_numpy_types(improvements),
            'training_history': convert_numpy_types(training_history),
            'test_timestamp': timestamp
        }
        
        import json
        with open(os.path.join(save_dir, 'advanced_optimization_results.json'), 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")

def main():
    """ä¸»å‡½æ•°"""
    tester = AdvancedOptimizationTester()
    
    # è¿è¡Œé«˜çº§ä¼˜åŒ–æµ‹è¯•
    print("ğŸ¯ é«˜çº§ä¼˜åŒ–æµ‹è¯• - å®ç°çœŸå®æ€§èƒ½æ”¹è¿›")
    print("ç›®æ ‡: è·å¾—çœŸå®æœ‰æ•ˆçš„å¹²æ‰°ç‡ã€æˆåŠŸç‡ç­‰å…³é”®æŒ‡æ ‡")
    
    baseline, final, history = tester.run_advanced_optimization_test(episodes=120)
    
    print("\nâœ… é«˜çº§ä¼˜åŒ–æµ‹è¯•å®Œæˆ!")
    print("ğŸ“ è¿™ä¸ªæµ‹è¯•å®ç°äº†:")
    print("  1. çœŸå®æœ‰æ•ˆçš„å¹²æ‰°æœºåˆ¶å’Œæ€§èƒ½æŒ‡æ ‡è®¡ç®—")
    print("  2. æ™ºèƒ½å¥–åŠ±è®¾è®¡å™¨çš„è‡ªé€‚åº”è°ƒæ•´")
    print("  3. ç»¼åˆæ€§èƒ½è¯„ä¼°å’Œè¯¦ç»†æ”¹è¿›åˆ†æ")
    print("  4. ä¸è®ºæ–‡ç›®æ ‡çš„ç²¾ç¡®å¯¹æ¯”å’Œä¼˜åŒ–å»ºè®®")

if __name__ == "__main__":
    main() 