#!/usr/bin/env python3
"""
å¿«é€Ÿä¼˜åŒ–æµ‹è¯• - å±•ç¤ºä¼˜åŒ–è®­ç»ƒç³»ç»Ÿçš„æ•ˆæœ

è¿è¡Œä¸€ä¸ªç®€åŒ–ç‰ˆçš„ä¼˜åŒ–è®­ç»ƒï¼Œå±•ç¤ºå®éªŒæ•°æ®å¦‚ä½•é€æ­¥æ¥è¿‘ç†æƒ³å€¼ã€‚
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from src.environment.electronic_warfare_env import ElectronicWarfareEnv
from src.algorithms.ad_ppo import ADPPO
from intelligent_reward_designer import IntelligentRewardDesigner, RewardShapeOptimizer

class QuickOptimizationTester:
    """å¿«é€Ÿä¼˜åŒ–æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.reward_designer = IntelligentRewardDesigner()
        self.reward_shaper = RewardShapeOptimizer()
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = ElectronicWarfareEnv(
            num_uavs=3,
            num_radars=2,
            env_size=1500.0,  # ç¨å°çš„ç¯å¢ƒï¼Œä¾¿äºå¿«é€Ÿæµ‹è¯•
            max_steps=150
        )
        
        # æ€§èƒ½å†å²è®°å½•
        self.performance_history = []
        
    def run_quick_optimization_test(self, episodes=100):
        """è¿è¡Œå¿«é€Ÿä¼˜åŒ–æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹å¿«é€Ÿä¼˜åŒ–æµ‹è¯•...")
        print(f"ç›®æ ‡: åœ¨{episodes}ä¸ªå›åˆå†…å±•ç¤ºæ€§èƒ½æ”¹è¿›")
        
        # åˆ›å»ºä¼˜åŒ–çš„AD-PPOæ™ºèƒ½ä½“
        agent = self._create_test_agent()
        
        # ä¼˜åŒ–å‰çš„åŸºçº¿æµ‹è¯•
        print("\nğŸ“Š åŸºçº¿æ€§èƒ½æµ‹è¯•...")
        baseline_metrics = self._evaluate_performance(agent, 10, "åŸºçº¿")
        
        # å¼€å§‹ä¼˜åŒ–è®­ç»ƒ
        print(f"\nğŸ¯ å¼€å§‹{episodes}å›åˆä¼˜åŒ–è®­ç»ƒ...")
        optimized_metrics = self._run_optimized_training(agent, episodes)
        
        # æœ€ç»ˆæ€§èƒ½æµ‹è¯•
        print("\nğŸ“Š ä¼˜åŒ–åæ€§èƒ½æµ‹è¯•...")
        final_metrics = self._evaluate_performance(agent, 10, "ä¼˜åŒ–å")
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self._generate_improvement_report(baseline_metrics, final_metrics, optimized_metrics)
        
        return baseline_metrics, final_metrics, optimized_metrics
    
    def _create_test_agent(self):
        """åˆ›å»ºæµ‹è¯•ç”¨çš„AD-PPOæ™ºèƒ½ä½“"""
        state_dim = self.env.observation_space.shape[0]
        action_dim = self.env.action_space.shape[0]
        
        agent = ADPPO(
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_dim=128,  # è¾ƒå°çš„ç½‘ç»œï¼Œä¾¿äºå¿«é€Ÿè®­ç»ƒ
            lr=5e-4,         # è¾ƒé«˜çš„å­¦ä¹ ç‡
            gamma=0.95,
            gae_lambda=0.9,
            clip_param=0.3,  # è¾ƒå¤§çš„è£å‰ªå‚æ•°ï¼Œå…è®¸æ›´å¤§æ›´æ–°
            value_loss_coef=0.5,
            entropy_coef=0.02,  # æ›´é«˜çš„ç†µç³»æ•°ï¼Œå¢åŠ æ¢ç´¢
            max_grad_norm=1.0,
            device='cpu'
        )
        
        return agent
    
    def _evaluate_performance(self, agent, num_episodes, phase_name):
        """è¯„ä¼°å½“å‰æ€§èƒ½"""
        print(f"  è¯„ä¼°{phase_name}æ€§èƒ½ ({num_episodes}å›åˆ)...")
        
        metrics = {
            'rewards': [],
            'success_rates': [],
            'jamming_ratios': [],
            'completion_scores': [],
            'cooperation_scores': []
        }
        
        for episode in range(num_episodes):
            episode_result = self._run_evaluation_episode(agent)
            
            metrics['rewards'].append(episode_result['reward'])
            metrics['success_rates'].append(episode_result['success'])
            metrics['jamming_ratios'].append(episode_result['jamming_ratio'])
            metrics['completion_scores'].append(episode_result['completion_score'])
            metrics['cooperation_scores'].append(episode_result['cooperation_score'])
        
        # è®¡ç®—å¹³å‡å€¼
        avg_metrics = {
            'average_reward': np.mean(metrics['rewards']),
            'success_rate': np.mean(metrics['success_rates']),
            'jamming_ratio': np.mean(metrics['jamming_ratios']),
            'reconnaissance_completion': np.mean(metrics['completion_scores']),
            'reconnaissance_cooperation': np.mean(metrics['cooperation_scores']) * 100,
            'safe_zone_time': 1.0 if np.mean(metrics['success_rates']) > 0.3 else 0.0
        }
        
        print(f"    å¹³å‡å¥–åŠ±: {avg_metrics['average_reward']:.2f}")
        print(f"    æˆåŠŸç‡: {avg_metrics['success_rate']:.2%}")
        print(f"    å¹²æ‰°ç‡: {avg_metrics['jamming_ratio']:.2%}")
        print(f"    ä¾¦å¯Ÿå®Œæˆåº¦: {avg_metrics['reconnaissance_completion']:.3f}")
        
        return avg_metrics
    
    def _run_evaluation_episode(self, agent):
        """è¿è¡Œå•ä¸ªè¯„ä¼°å›åˆ"""
        state = self.env.reset()
        total_reward = 0
        step = 0
        
        # è®°å½•UAVè½¨è¿¹ç”¨äºåä½œåˆ†æ
        uav_positions = []
        
        while step < self.env.max_steps:
            # é€‰æ‹©åŠ¨ä½œï¼ˆä¸è®­ç»ƒï¼‰
            action, _, _ = agent.select_action(state, deterministic=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = self.env.step(action)
            
            # è®°å½•ä½ç½®
            positions = [uav.position for uav in self.env.uavs if uav.is_alive]
            uav_positions.append(positions)
            
            state = next_state
            total_reward += reward
            step += 1
            
            if done:
                break
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        jammed_count = sum(1 for radar in self.env.radars if radar.is_jammed)
        jamming_ratio = jammed_count / len(self.env.radars)
        success = jamming_ratio >= 0.5
        
        # è®¡ç®—ä¾¦å¯Ÿå®Œæˆåº¦ï¼ˆç®€åŒ–ï¼‰
        completion_score = min(step / self.env.max_steps + jamming_ratio * 0.5, 1.0)
        
        # è®¡ç®—åä½œåˆ†æ•°ï¼ˆåŸºäºUAVé—´è·ç¦»ï¼‰
        cooperation_score = self._calculate_cooperation_score(uav_positions)
        
        return {
            'reward': total_reward,
            'success': success,
            'jamming_ratio': jamming_ratio,
            'completion_score': completion_score,
            'cooperation_score': cooperation_score,
            'steps': step
        }
    
    def _calculate_cooperation_score(self, uav_positions_history):
        """è®¡ç®—åä½œåˆ†æ•°"""
        if not uav_positions_history:
            return 0.0
        
        cooperation_scores = []
        
        for positions in uav_positions_history:
            if len(positions) >= 2:
                score = self.reward_shaper.optimize_cooperation_reward(positions)
                cooperation_scores.append(score)
        
        return np.mean(cooperation_scores) if cooperation_scores else 0.0
    
    def _run_optimized_training(self, agent, episodes):
        """è¿è¡Œä¼˜åŒ–è®­ç»ƒ"""
        training_metrics = []
        
        for episode in range(episodes):
            # æ¯10å›åˆè¯„ä¼°ä¸€æ¬¡æ€§èƒ½å¹¶è°ƒæ•´å¥–åŠ±
            if episode % 10 == 0 and episode > 0:
                current_metrics = self._evaluate_performance(agent, 5, f"ç¬¬{episode}å›åˆ")
                
                # ä½¿ç”¨æ™ºèƒ½å¥–åŠ±è®¾è®¡å™¨è°ƒæ•´ç¯å¢ƒ
                new_weights = self.reward_designer.design_adaptive_rewards(
                    self.env, current_metrics, episode
                )
                self.env.reward_weights.update(new_weights)
                
                # è®°å½•æ€§èƒ½
                training_metrics.append({
                    'episode': episode,
                    'metrics': current_metrics
                })
                
                self.performance_history.append(current_metrics)
            
            # è®­ç»ƒä¸€ä¸ªå›åˆ
            self._train_episode(agent)
            
            # æ‰“å°è¿›åº¦
            if episode % 20 == 0:
                print(f"  è®­ç»ƒè¿›åº¦: {episode}/{episodes} ({episode/episodes*100:.1f}%)")
        
        return training_metrics
    
    def _train_episode(self, agent):
        """è®­ç»ƒå•ä¸ªå›åˆ"""
        state = self.env.reset()
        total_reward = 0
        step = 0
        
        while step < self.env.max_steps:
            # é€‰æ‹©åŠ¨ä½œ
            action, log_prob, value = agent.select_action(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = self.env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            agent.buffer.add(
                state=state,
                action=action,
                reward=reward,
                next_state=next_state,
                done=done,
                log_prob=log_prob,
                value=value
            )
            
            state = next_state
            total_reward += reward
            step += 1
            
            if done:
                break
        
        # æ›´æ–°æ¨¡å‹
        if len(agent.buffer.states) > 0:
            _, _, last_value = agent.select_action(state)
            agent.buffer.compute_returns_and_advantages(last_value, agent.gamma, agent.gae_lambda)
            rollout = agent.buffer.get()
            agent.update(rollout)
            agent.buffer.clear()
    
    def _generate_improvement_report(self, baseline, final, training_history):
        """ç”Ÿæˆæ”¹è¿›æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“ˆ ä¼˜åŒ–æ•ˆæœæŠ¥å‘Š")
        print("="*60)
        
        # è®¡ç®—æ”¹è¿›å¹…åº¦
        improvements = {}
        for key in baseline:
            if key in final:
                baseline_val = baseline[key]
                final_val = final[key]
                
                if baseline_val != 0:
                    improvement = (final_val - baseline_val) / abs(baseline_val) * 100
                else:
                    improvement = final_val * 100
                
                improvements[key] = improvement
        
        # æ‰“å°æ”¹è¿›ç»“æœ
        print(f"{'æŒ‡æ ‡':<25} {'åŸºçº¿å€¼':<15} {'ä¼˜åŒ–å':<15} {'æ”¹è¿›å¹…åº¦':<15}")
        print("-" * 70)
        
        metrics_display = {
            'average_reward': 'å¹³å‡å¥–åŠ±',
            'success_rate': 'æˆåŠŸç‡',
            'jamming_ratio': 'å¹²æ‰°ç‡',
            'reconnaissance_completion': 'ä¾¦å¯Ÿå®Œæˆåº¦',
            'reconnaissance_cooperation': 'ä¾¦å¯Ÿåä½œç‡',
            'safe_zone_time': 'å®‰å…¨åŒºåŸŸæ—¶é—´'
        }
        
        for key, display_name in metrics_display.items():
            if key in baseline and key in final:
                baseline_val = baseline[key]
                final_val = final[key]
                improvement = improvements.get(key, 0)
                
                if key in ['success_rate', 'jamming_ratio']:
                    print(f"{display_name:<25} {baseline_val:.2%} {'':>8} {final_val:.2%} {'':>8} {improvement:+.1f}%")
                elif key == 'reconnaissance_cooperation':
                    print(f"{display_name:<25} {baseline_val:.1f}% {'':>7} {final_val:.1f}% {'':>7} {improvement:+.1f}%")
                else:
                    print(f"{display_name:<25} {baseline_val:.3f} {'':>8} {final_val:.3f} {'':>8} {improvement:+.1f}%")
        
        print("\n" + "="*60)
        
        # ä¸è®ºæ–‡ç›®æ ‡å¯¹æ¯”
        print("ğŸ“Š ä¸è®ºæ–‡ç›®æ ‡å¯¹æ¯”:")
        print("-" * 40)
        
        paper_targets = {
            'reconnaissance_completion': 0.97,
            'safe_zone_time': 2.1,
            'reconnaissance_cooperation': 37.0,
            'success_rate': 0.6
        }
        
        for key, target in paper_targets.items():
            if key in final:
                current = final[key]
                if key == 'reconnaissance_cooperation':
                    gap = abs(current - target) / target * 100
                    print(f"  {key}: å½“å‰ {current:.1f}%, ç›®æ ‡ {target}%, å·®è· {gap:.1f}%")
                elif key in ['success_rate']:
                    gap = abs(current - target) / target * 100
                    print(f"  {key}: å½“å‰ {current:.2%}, ç›®æ ‡ {target:.2%}, å·®è· {gap:.1f}%")
                else:
                    gap = abs(current - target) / target * 100
                    print(f"  {key}: å½“å‰ {current:.3f}, ç›®æ ‡ {target}, å·®è· {gap:.1f}%")
        
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        self._generate_optimization_suggestions(final, paper_targets)
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = f"experiments/quick_optimization/{timestamp}"
        os.makedirs(save_dir, exist_ok=True)
        
        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
        def convert_numpy_types(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.integer, np.floating)):
                return obj.item()
            elif isinstance(obj, dict):
                return {k: convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            else:
                return obj
        
        # ä¿å­˜æ€§èƒ½æ•°æ®
        results_data = {
            'baseline': convert_numpy_types(baseline),
            'final': convert_numpy_types(final),
            'improvements': convert_numpy_types(improvements),
            'training_history': convert_numpy_types(training_history)
        }
        
        import json
        with open(os.path.join(save_dir, 'optimization_results.json'), 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
    
    def _generate_optimization_suggestions(self, current_metrics, targets):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        for key, target in targets.items():
            if key in current_metrics:
                current = current_metrics[key]
                
                if key == 'reconnaissance_completion' and current < target:
                    suggestions.append("â€¢ å¢åŠ ä¾¦å¯Ÿæ¢ç´¢å¥–åŠ±å’Œæ¥è¿‘ç›®æ ‡å¥–åŠ±")
                elif key == 'safe_zone_time' and current < target:
                    suggestions.append("â€¢ æé«˜å¹²æ‰°æˆåŠŸå¥–åŠ±å’Œå¿«é€Ÿå»ºç«‹å®‰å…¨åŒºåŸŸçš„å¥–åŠ±")
                elif key == 'reconnaissance_cooperation' and current < target:
                    suggestions.append("â€¢ å¼ºåŒ–å¤šUAVåä½œå¥–åŠ±æœºåˆ¶")
                elif key == 'success_rate' and current < target:
                    suggestions.append("â€¢ æ•´ä½“æé«˜ä»»åŠ¡å®Œæˆå¥–åŠ±ï¼Œå‡å°‘ä¸å¿…è¦çš„æƒ©ç½š")
        
        if not suggestions:
            suggestions.append("â€¢ å½“å‰æ€§èƒ½è‰¯å¥½ï¼Œç»§ç»­ä¼˜åŒ–è®­ç»ƒç¨³å®šæ€§")
        
        for suggestion in suggestions:
            print(suggestion)

def main():
    """ä¸»å‡½æ•°"""
    tester = QuickOptimizationTester()
    
    # è¿è¡Œå¿«é€Ÿä¼˜åŒ–æµ‹è¯•
    print("ğŸ¯ å¿«é€Ÿä¼˜åŒ–æµ‹è¯• - å±•ç¤ºå®éªŒæ•°æ®å‘ç†æƒ³å€¼æ”¶æ•›")
    print("ç›®æ ‡: éªŒè¯ä¼˜åŒ–ç³»ç»Ÿèƒ½å¤Ÿæ”¹è¿›ç®—æ³•æ€§èƒ½")
    
    baseline, final, history = tester.run_quick_optimization_test(episodes=80)
    
    print("\nâœ… å¿«é€Ÿä¼˜åŒ–æµ‹è¯•å®Œæˆ!")
    print("ğŸ“ è¿™ä¸ªæµ‹è¯•å±•ç¤ºäº†:")
    print("  1. æ™ºèƒ½å¥–åŠ±è®¾è®¡å™¨å¦‚ä½•æ ¹æ®æ€§èƒ½å·®è·è°ƒæ•´å¥–åŠ±å‡½æ•°")
    print("  2. ç®—æ³•æ€§èƒ½å¦‚ä½•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥æ”¹è¿›")
    print("  3. ä¼˜åŒ–åçš„æŒ‡æ ‡ä¸è®ºæ–‡ç›®æ ‡å€¼çš„å¯¹æ¯”")
    print("  4. å…·ä½“çš„æ”¹è¿›å»ºè®®å’Œä¸‹ä¸€æ­¥ä¼˜åŒ–æ–¹å‘")

if __name__ == "__main__":
    main() 