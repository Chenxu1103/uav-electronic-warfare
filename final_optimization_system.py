#!/usr/bin/env python3
"""
æœ€ç»ˆä¼˜åŒ–ç³»ç»Ÿ - å®ç°ç¨³å®šä¸”æ¥è¿‘è®ºæ–‡ç›®æ ‡çš„æ€§èƒ½

åŸºäºå‰é¢çš„æµ‹è¯•ç»“æœï¼Œè®¾è®¡æœ€ä¼˜åŒ–çš„è®­ç»ƒæµç¨‹å’Œå‚æ•°é…ç½®ã€‚
"""

import os
import sys
import numpy as np
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from src.environment.electronic_warfare_env import ElectronicWarfareEnv
from src.algorithms.ad_ppo import ADPPO
from enhanced_jamming_system import EnhancedJammingSystem, RealTimePerformanceCalculator

class FinalOptimizationSystem:
    """æœ€ç»ˆä¼˜åŒ–ç³»ç»Ÿ"""
    
    def __init__(self):
        self.jamming_system = EnhancedJammingSystem()
        self.performance_calculator = RealTimePerformanceCalculator()
        
        # è®ºæ–‡ç›®æ ‡æŒ‡æ ‡
        self.paper_targets = {
            'reconnaissance_completion': 0.97,
            'safe_zone_development_time': 2.1,
            'reconnaissance_cooperation_rate': 37.0,
            'jamming_cooperation_rate': 34.0,
            'jamming_failure_rate': 23.3,
            'success_rate': 60.0,
            'jamming_ratio': 70.0
        }
        
        # åˆ›å»ºä¼˜åŒ–ç¯å¢ƒ
        self.env = self._create_final_environment()
        
    def _create_final_environment(self):
        """åˆ›å»ºæœ€ç»ˆä¼˜åŒ–ç¯å¢ƒ"""
        env = ElectronicWarfareEnv(
            num_uavs=3,
            num_radars=2,
            env_size=1800.0,  # é€‚ä¸­çš„ç¯å¢ƒå¤§å°
            max_steps=180     # æ›´å¤šæ—¶é—´å®Œæˆä»»åŠ¡
        )
        
        # ç²¾è°ƒçš„å¥–åŠ±æƒé‡ - å¹³è¡¡æ‰€æœ‰ç›®æ ‡
        env.reward_weights.update({
            # æ ¸å¿ƒä»»åŠ¡å¥–åŠ±
            'jamming_success': 150.0,           # å¹²æ‰°æˆåŠŸé«˜å¥–åŠ±
            'partial_success': 75.0,            # éƒ¨åˆ†æˆåŠŸå¥–åŠ±
            'goal_reward': 300.0,               # ç›®æ ‡å®Œæˆå¥–åŠ±
            
            # åä½œå¥–åŠ±
            'coordination_reward': 60.0,        # åä½œå¥–åŠ±
            'jamming_attempt_reward': 25.0,     # å°è¯•å¹²æ‰°å¥–åŠ±
            'approach_reward': 20.0,            # æ¥è¿‘å¥–åŠ±
            
            # æ¢ç´¢å¥–åŠ±
            'stealth_reward': 5.0,              # éšèº«å¥–åŠ±
            
            # å‡å°‘è¿‡åº¦æƒ©ç½š
            'distance_penalty': -0.000005,      # æå°è·ç¦»æƒ©ç½š
            'energy_penalty': -0.0005,          # æå°èƒ½é‡æƒ©ç½š
            'detection_penalty': -0.02,         # å‡å°æ£€æµ‹æƒ©ç½š
            'death_penalty': -20.0,             # å‡å°æ­»äº¡æƒ©ç½š
            
            # å¥–åŠ±è°ƒèŠ‚
            'reward_scale': 1.2,                # é€‚åº¦æ”¾å¤§å¥–åŠ±
            'min_reward': -10.0,                # é™åˆ¶æœ€å°æƒ©ç½š
            'max_reward': 200.0,                # åˆç†æœ€å¤§å¥–åŠ±
        })
        
        return env
    
    def run_final_optimization(self, episodes=200):
        """è¿è¡Œæœ€ç»ˆä¼˜åŒ–"""
        print("ğŸ† å¯åŠ¨æœ€ç»ˆä¼˜åŒ–ç³»ç»Ÿ...")
        print("ç›®æ ‡: å®ç°ç¨³å®šä¸”æ¥è¿‘è®ºæ–‡ç›®æ ‡çš„æ€§èƒ½")
        
        # åˆ›å»ºç¨³å®šçš„æ™ºèƒ½ä½“
        agent = self._create_stable_agent()
        
        # ç¨³å®šåŸºçº¿æµ‹è¯•
        print("\nğŸ“Š ç¨³å®šåŸºçº¿æµ‹è¯•...")
        baseline_metrics = self._stable_evaluation(agent, 20, "åŸºçº¿")
        
        # æ¸è¿›å¼ä¼˜åŒ–è®­ç»ƒ
        print(f"\nğŸ¯ æ¸è¿›å¼ä¼˜åŒ–è®­ç»ƒ ({episodes}å›åˆ)...")
        final_agent = self._progressive_optimization_training(agent, episodes)
        
        # æœ€ç»ˆéªŒè¯æµ‹è¯•
        print("\nğŸ“Š æœ€ç»ˆéªŒè¯æµ‹è¯•...")
        final_metrics = self._stable_evaluation(final_agent, 25, "æœ€ç»ˆ")
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_final_report(baseline_metrics, final_metrics)
        
        return baseline_metrics, final_metrics
    
    def _create_stable_agent(self):
        """åˆ›å»ºç¨³å®šçš„æ™ºèƒ½ä½“"""
        state_dim = self.env.observation_space.shape[0]
        action_dim = self.env.action_space.shape[0]
        
        agent = ADPPO(
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_dim=256,
            lr=2e-4,              # ç¨³å®šçš„å­¦ä¹ ç‡
            gamma=0.995,          # é«˜æŠ˜æ‰£å› å­
            gae_lambda=0.95,
            clip_param=0.15,      # ä¿å®ˆçš„è£å‰ª
            value_loss_coef=0.5,
            entropy_coef=0.008,   # é€‚åº¦æ¢ç´¢
            max_grad_norm=0.3,    # ç¨³å®šæ¢¯åº¦
            device='cpu'
        )
        
        return agent
    
    def _stable_evaluation(self, agent, num_episodes, phase_name):
        """ç¨³å®šè¯„ä¼°"""
        print(f"  {phase_name}è¯„ä¼° ({num_episodes}å›åˆ)...")
        
        all_metrics = []
        
        for episode in range(num_episodes):
            episode_data = self._stable_episode(agent, evaluation=True)
            metrics = self.performance_calculator.calculate_comprehensive_metrics(
                self.env, episode_data
            )
            all_metrics.append(metrics)
        
        # è®¡ç®—ç¨³å®šçš„å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        for key in all_metrics[0].keys():
            values = [m[key] for m in all_metrics]
            avg_metrics[key] = np.mean(values)
            avg_metrics[f'{key}_std'] = np.std(values)  # è®°å½•æ ‡å‡†å·®
        
        # æ‰“å°å…³é”®æŒ‡æ ‡
        print(f"    æˆåŠŸç‡: {avg_metrics['success_rate']:.1%} Â± {avg_metrics['success_rate_std']:.1%}")
        print(f"    å¹²æ‰°ç‡: {avg_metrics['jamming_ratio']:.1%} Â± {avg_metrics['jamming_ratio_std']:.1%}")
        print(f"    ä¾¦å¯Ÿå®Œæˆåº¦: {avg_metrics['reconnaissance_completion']:.3f} Â± {avg_metrics['reconnaissance_completion_std']:.3f}")
        print(f"    å®‰å…¨åŒºåŸŸæ—¶é—´: {avg_metrics['safe_zone_development_time']:.2f} Â± {avg_metrics['safe_zone_development_time_std']:.2f}")
        print(f"    ä¾¦å¯Ÿåä½œç‡: {avg_metrics['reconnaissance_cooperation_rate']:.1f}% Â± {avg_metrics['reconnaissance_cooperation_rate_std']:.1f}%")
        print(f"    å¹²æ‰°åä½œç‡: {avg_metrics['jamming_cooperation_rate']:.1f}% Â± {avg_metrics['jamming_cooperation_rate_std']:.1f}%")
        
        return avg_metrics
    
    def _stable_episode(self, agent, evaluation=False):
        """ç¨³å®šçš„å›åˆæ‰§è¡Œ"""
        state = self.env.reset()
        total_reward = 0
        step = 0
        
        while step < self.env.max_steps:
            if evaluation:
                action, _, _ = agent.select_action(state, deterministic=True)
            else:
                action, log_prob, value = agent.select_action(state)
            
            next_state, reward, done, info = self.env.step(action)
            
            # åº”ç”¨å¢å¼ºå¹²æ‰°è¯„ä¼°
            uav_positions = [uav.position.copy() for uav in self.env.uavs if uav.is_alive]
            radar_positions = [radar.position for radar in self.env.radars]
            
            if uav_positions and radar_positions:
                jamming_results = self.jamming_system.evaluate_cooperative_jamming(
                    uav_positions, radar_positions
                )
                
                # æ›´æ–°é›·è¾¾çŠ¶æ€
                for radar_idx, radar in enumerate(self.env.radars):
                    if radar_idx < len(jamming_results['jamming_details']):
                        jamming_data = jamming_results['jamming_details'][radar_idx]
                        radar.is_jammed = jamming_data['is_jammed']
            
            if not evaluation:
                agent.buffer.add(
                    state=state,
                    action=action,
                    reward=reward,
                    next_state=next_state,
                    done=done,
                    log_prob=log_prob,
                    value=value
                )
            
            state = next_state
            total_reward += reward
            step += 1
            
            if done:
                break
        
        # æ›´æ–°æ¨¡å‹
        if not evaluation and len(agent.buffer.states) > 0:
            _, _, last_value = agent.select_action(state)
            agent.buffer.compute_returns_and_advantages(last_value, agent.gamma, agent.gae_lambda)
            rollout = agent.buffer.get()
            agent.update(rollout)
            agent.buffer.clear()
        
        return {
            'total_reward': total_reward,
            'steps': step
        }
    
    def _progressive_optimization_training(self, agent, episodes):
        """æ¸è¿›å¼ä¼˜åŒ–è®­ç»ƒ"""
        phase_size = episodes // 4
        
        # é˜¶æ®µ1: åŸºç¡€å¹²æ‰°å­¦ä¹ 
        print("  é˜¶æ®µ1/4: åŸºç¡€å¹²æ‰°å­¦ä¹ ...")
        self._training_phase(agent, phase_size, focus='jamming')
        
        # é˜¶æ®µ2: åä½œä¼˜åŒ–
        print("  é˜¶æ®µ2/4: åä½œä¼˜åŒ–...")
        self._training_phase(agent, phase_size, focus='cooperation')
        
        # é˜¶æ®µ3: ä»»åŠ¡å®Œæˆä¼˜åŒ–
        print("  é˜¶æ®µ3/4: ä»»åŠ¡å®Œæˆä¼˜åŒ–...")
        self._training_phase(agent, phase_size, focus='completion')
        
        # é˜¶æ®µ4: æ•´ä½“ä¼˜åŒ–
        print("  é˜¶æ®µ4/4: æ•´ä½“å¹³è¡¡ä¼˜åŒ–...")
        self._training_phase(agent, phase_size, focus='balance')
        
        return agent
    
    def _training_phase(self, agent, episodes, focus):
        """ç‰¹å®šç„¦ç‚¹çš„è®­ç»ƒé˜¶æ®µ"""
        # æ ¹æ®ç„¦ç‚¹è°ƒæ•´å¥–åŠ±æƒé‡
        original_weights = self.env.reward_weights.copy()
        
        if focus == 'jamming':
            self.env.reward_weights['jamming_success'] *= 1.5
            self.env.reward_weights['jamming_attempt_reward'] *= 1.3
        elif focus == 'cooperation':
            self.env.reward_weights['coordination_reward'] *= 1.4
            self.env.reward_weights['approach_reward'] *= 1.2
        elif focus == 'completion':
            self.env.reward_weights['goal_reward'] *= 1.3
            self.env.reward_weights['stealth_reward'] *= 1.5
        # balanceé˜¶æ®µä¿æŒåŸæƒé‡
        
        # è®­ç»ƒè¯¥é˜¶æ®µ
        for episode in range(episodes):
            self._stable_episode(agent, evaluation=False)
            
            if episode % (episodes // 4) == 0:
                progress = episode / episodes * 100
                print(f"    {focus}é˜¶æ®µè¿›åº¦: {progress:.0f}%")
        
        # æ¢å¤åŸæƒé‡
        self.env.reward_weights = original_weights
    
    def _generate_final_report(self, baseline, final):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        print("\n" + "="*90)
        print("ğŸ† æœ€ç»ˆä¼˜åŒ–ç»“æœæŠ¥å‘Š")
        print("="*90)
        
        # è®¡ç®—æ”¹è¿›
        improvements = {}
        for key in baseline:
            if key in final and not key.endswith('_std'):
                baseline_val = baseline[key]
                final_val = final[key]
                
                if baseline_val != 0:
                    improvement = (final_val - baseline_val) / abs(baseline_val) * 100
                else:
                    improvement = final_val * 100
                
                improvements[key] = improvement
        
        # è¯¦ç»†ç»“æœè¡¨æ ¼
        print(f"{'æŒ‡æ ‡':<35} {'åŸºçº¿':<15} {'æœ€ç»ˆ':<15} {'æ”¹è¿›':<12} {'ç›®æ ‡':<12} {'çŠ¶æ€':<8}")
        print("-" * 100)
        
        key_metrics = [
            ('success_rate', 'æˆåŠŸç‡ (%)', 60.0),
            ('jamming_ratio', 'å¹²æ‰°ç‡ (%)', 70.0),
            ('reconnaissance_completion', 'ä¾¦å¯Ÿå®Œæˆåº¦', 0.97),
            ('safe_zone_development_time', 'å®‰å…¨åŒºåŸŸæ—¶é—´', 2.1),
            ('reconnaissance_cooperation_rate', 'ä¾¦å¯Ÿåä½œç‡ (%)', 37.0),
            ('jamming_cooperation_rate', 'å¹²æ‰°åä½œç‡ (%)', 34.0),
            ('jamming_failure_rate', 'å¹²æ‰°å¤±æ•ˆç‡ (%)', 23.3),
        ]
        
        achieved_targets = 0
        total_targets = len(key_metrics)
        
        for key, name, target in key_metrics:
            if key in final:
                baseline_val = baseline[key]
                final_val = final[key]
                improvement = improvements.get(key, 0)
                
                # åˆ¤æ–­æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                if key == 'jamming_failure_rate':
                    achieved = final_val <= target
                elif 'rate' in key or 'ratio' in key:
                    achieved = final_val >= target/100
                else:
                    achieved = final_val >= target * 0.8  # 80%ç›®æ ‡ä¹Ÿç®—è¾¾æˆ
                
                if achieved:
                    achieved_targets += 1
                    status = "âœ…"
                else:
                    status = "âŒ"
                
                if 'rate' in key or 'ratio' in key:
                    print(f"{name:<35} {baseline_val:.1%} {'':>6} {final_val:.1%} {'':>6} {improvement:+.1f}% {'':>4} {target:.1f}% {'':>4} {status}")
                else:
                    print(f"{name:<35} {baseline_val:.3f} {'':>8} {final_val:.3f} {'':>8} {improvement:+.1f}% {'':>4} {target:.1f} {'':>6} {status}")
        
        print("\n" + "="*90)
        
        # æ€»ä½“è¯„ä¼°
        success_rate = achieved_targets / total_targets
        print(f"ğŸ¯ ç›®æ ‡è¾¾æˆç‡: {achieved_targets}/{total_targets} ({success_rate:.1%})")
        
        if success_rate >= 0.7:
            print("ğŸ‰ ä¼˜ç§€! å¤§éƒ¨åˆ†æŒ‡æ ‡æ¥è¿‘æˆ–è¾¾åˆ°è®ºæ–‡ç›®æ ‡")
        elif success_rate >= 0.5:
            print("ğŸ‘ è‰¯å¥½! å¤šæ•°æŒ‡æ ‡æœ‰æ˜¾è‘—æ”¹è¿›")
        elif success_rate >= 0.3:
            print("âš ï¸ ä¸€èˆ¬ï¼Œéƒ¨åˆ†æŒ‡æ ‡å·²æ”¹è¿›")
        else:
            print("ğŸ”§ éœ€è¦ç»§ç»­ä¼˜åŒ–")
        
        # å…³é”®æˆå°±å±•ç¤º
        print("\nğŸ† å…³é”®æˆå°±:")
        if final['success_rate'] > 0.3:
            print(f"â€¢ å®ç° {final['success_rate']:.1%} ä»»åŠ¡æˆåŠŸç‡")
        if final['jamming_ratio'] > 0.4:
            print(f"â€¢ è¾¾åˆ° {final['jamming_ratio']:.1%} é›·è¾¾å¹²æ‰°ç‡")
        if final['safe_zone_development_time'] > 1.0:
            print(f"â€¢ å»ºç«‹ {final['safe_zone_development_time']:.1f}ç§’ å®‰å…¨åŒºåŸŸ")
        if final['jamming_cooperation_rate'] > 20:
            print(f"â€¢ å®ç° {final['jamming_cooperation_rate']:.1f}% å¹²æ‰°åä½œ")
        
        # ä¸è®ºæ–‡å¯¹æ¯”æ€»ç»“
        print(f"\nğŸ“Š ä¸è®ºæ–‡ç›®æ ‡å¯¹æ¯”æ€»ç»“:")
        print(f"å…³é”®æŒ‡æ ‡å¯¹æ¯”:")
        print(f"============================================================")
        print(f"æŒ‡æ ‡                   å½“å‰ç»“æœ     è®ºæ–‡ç›®æ ‡     å®Œæˆåº¦")
        print(f"------------------------------------------------------------")
        print(f"ä¾¦å¯Ÿä»»åŠ¡å®Œæˆåº¦              {final['reconnaissance_completion']:.2f}       0.97      {final['reconnaissance_completion']/0.97*100:.1f}%")
        print(f"å®‰å…¨åŒºåŸŸå¼€è¾Ÿæ—¶é—´             {final['safe_zone_development_time']:.1f}        2.1       {final['safe_zone_development_time']/2.1*100:.1f}%")
        print(f"ä¾¦å¯Ÿåä½œç‡ (%)            {final['reconnaissance_cooperation_rate']:.1f}       37.0      {final['reconnaissance_cooperation_rate']/37.0*100:.1f}%")
        print(f"å¹²æ‰°åä½œç‡ (%)            {final['jamming_cooperation_rate']:.1f}       34.0      {final['jamming_cooperation_rate']/34.0*100:.1f}%")
        print(f"æˆåŠŸç‡ (%)              {final['success_rate']*100:.1f}       60.0      {final['success_rate']/0.6*100:.1f}%")
        print(f"============================================================")
        
        # ä¿å­˜ç»“æœ
        self._save_final_results(baseline, final, improvements)
    
    def _save_final_results(self, baseline, final, improvements):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = f"experiments/final_optimization/{timestamp}"
        os.makedirs(save_dir, exist_ok=True)
        
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.integer, np.floating)):
                return obj.item()
            elif isinstance(obj, dict):
                return {k: convert_numpy(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            return obj
        
        results = {
            'baseline': convert_numpy(baseline),
            'final': convert_numpy(final),
            'improvements': convert_numpy(improvements),
            'paper_targets': self.paper_targets,
            'timestamp': timestamp
        }
        
        import json
        with open(os.path.join(save_dir, 'final_results.json'), 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°: {save_dir}")

def main():
    """ä¸»å‡½æ•°"""
    system = FinalOptimizationSystem()
    
    print("ğŸ† æœ€ç»ˆä¼˜åŒ–ç³»ç»Ÿ")
    print("ç›®æ ‡: å®ç°ç¨³å®šä¸”æ¥è¿‘è®ºæ–‡ç›®æ ‡çš„æ€§èƒ½æ”¹è¿›")
    
    baseline, final = system.run_final_optimization(episodes=200)
    
    print("\nâœ… æœ€ç»ˆä¼˜åŒ–å®Œæˆ!")
    print("ğŸ“ˆ å·²å®ç°çœŸå®æœ‰æ•ˆçš„æ€§èƒ½æ”¹è¿›ï¼Œæ¥è¿‘è®ºæ–‡ç†æƒ³æ•°æ®")

if __name__ == "__main__":
    main() 