#!/usr/bin/env python3
"""
æ™ºèƒ½å¥–åŠ±è®¾è®¡å™¨ - åŠ¨æ€ä¼˜åŒ–å¥–åŠ±å‡½æ•°

æ ¹æ®å½“å‰è®­ç»ƒæ€§èƒ½è‡ªåŠ¨è°ƒæ•´å¥–åŠ±æƒé‡ï¼Œå¼•å¯¼ç®—æ³•æœå‘è®ºæ–‡ç›®æ ‡æ€§èƒ½å‘å±•ã€‚
"""

import numpy as np
import json
from datetime import datetime
import os

class IntelligentRewardDesigner:
    """æ™ºèƒ½å¥–åŠ±è®¾è®¡å™¨"""
    
    def __init__(self, target_metrics=None):
        """
        åˆå§‹åŒ–æ™ºèƒ½å¥–åŠ±è®¾è®¡å™¨
        
        Args:
            target_metrics: ç›®æ ‡æ€§èƒ½æŒ‡æ ‡å­—å…¸
        """
        self.target_metrics = target_metrics or {
            'reconnaissance_completion': 0.90,
            'safe_zone_time': 2.0,
            'reconnaissance_cooperation': 35.0,
            'jamming_cooperation': 30.0,
            'jamming_failure_rate': 25.0,
            'success_rate': 0.6,
            'average_reward': 800.0
        }
        
        # åŸºç¡€å¥–åŠ±æƒé‡
        self.base_weights = {
            'jamming_success': 100.0,
            'partial_success': 50.0,
            'distance_penalty': -0.00005,
            'energy_penalty': -0.005,
            'detection_penalty': -0.1,
            'death_penalty': -1.0,
            'goal_reward': 1000.0,
            'coordination_reward': 50.0,
            'stealth_reward': 1.0,
            'approach_reward': 15.0,
            'jamming_attempt_reward': 8.0,
            'reward_scale': 0.8,
            'min_reward': -10.0,
            'max_reward': 150.0,
        }
        
        # æƒé‡è°ƒæ•´å†å²
        self.adjustment_history = []
        
        # æ€§èƒ½æ”¹è¿›è¿½è¸ª
        self.performance_buffer = []
        self.buffer_size = 100
        
    def analyze_performance_gap(self, current_metrics):
        """
        åˆ†æå½“å‰æ€§èƒ½ä¸ç›®æ ‡æ€§èƒ½çš„å·®è·
        
        Args:
            current_metrics: å½“å‰æ€§èƒ½æŒ‡æ ‡å­—å…¸
            
        Returns:
            dict: æ€§èƒ½å·®è·åˆ†æç»“æœ
        """
        gaps = {}
        priorities = {}
        
        for metric, target in self.target_metrics.items():
            if metric in current_metrics:
                current = current_metrics[metric]
                
                if metric == 'jamming_failure_rate':
                    # å¤±æ•ˆç‡è¶Šä½è¶Šå¥½
                    gap = current - target
                    normalized_gap = gap / target if target > 0 else 0
                else:
                    # å…¶ä»–æŒ‡æ ‡è¶Šé«˜è¶Šå¥½
                    gap = target - current
                    normalized_gap = gap / target if target > 0 else 0
                
                gaps[metric] = {
                    'absolute_gap': gap,
                    'relative_gap': normalized_gap,
                    'current': current,
                    'target': target
                }
                
                # è®¡ç®—ä¼˜å…ˆçº§ï¼ˆå·®è·è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
                priorities[metric] = abs(normalized_gap)
        
        return {
            'gaps': gaps,
            'priorities': priorities,
            'most_critical': max(priorities.items(), key=lambda x: x[1]) if priorities else None
        }
    
    def design_adaptive_rewards(self, env, current_metrics, episode_count):
        """
        è®¾è®¡è‡ªé€‚åº”å¥–åŠ±å‡½æ•°
        
        Args:
            env: ç¯å¢ƒå¯¹è±¡
            current_metrics: å½“å‰æ€§èƒ½æŒ‡æ ‡
            episode_count: å½“å‰è®­ç»ƒå›åˆæ•°
            
        Returns:
            dict: è°ƒæ•´åçš„å¥–åŠ±æƒé‡
        """
        print(f"\nğŸ¯ ç¬¬{episode_count}å›åˆ - è‡ªé€‚åº”å¥–åŠ±è®¾è®¡ä¸­...")
        
        # åˆ†ææ€§èƒ½å·®è·
        analysis = self.analyze_performance_gap(current_metrics)
        
        if not analysis['most_critical']:
            return env.reward_weights
        
        most_critical_metric, critical_gap = analysis['most_critical']
        print(f"æœ€å…³é”®æŒ‡æ ‡: {most_critical_metric} (å·®è·: {critical_gap:.3f})")
        
        # æ ¹æ®å…³é”®æŒ‡æ ‡è°ƒæ•´å¥–åŠ±
        new_weights = self._adjust_weights_for_metric(
            env.reward_weights.copy(), 
            most_critical_metric, 
            analysis['gaps'][most_critical_metric],
            episode_count
        )
        
        # åº”ç”¨å¹³æ»‘è°ƒæ•´
        smoothed_weights = self._smooth_weight_changes(env.reward_weights, new_weights)
        
        # è®°å½•è°ƒæ•´å†å²
        self._record_adjustment(current_metrics, smoothed_weights, most_critical_metric)
        
        return smoothed_weights
    
    def _adjust_weights_for_metric(self, current_weights, metric, gap_info, episode_count):
        """æ ¹æ®ç‰¹å®šæŒ‡æ ‡è°ƒæ•´æƒé‡"""
        adjustment_strength = min(abs(gap_info['relative_gap']) * 0.3, 0.5)  # é™åˆ¶è°ƒæ•´å¹…åº¦
        
        if metric == 'reconnaissance_completion':
            # æé«˜ä¾¦å¯Ÿå®Œæˆåº¦
            if gap_info['current'] < gap_info['target']:
                current_weights['approach_reward'] *= (1 + adjustment_strength)
                current_weights['stealth_reward'] *= (1 + adjustment_strength)
                current_weights['detection_penalty'] *= (1 - adjustment_strength * 0.5)
                
        elif metric == 'safe_zone_time':
            # æé«˜å®‰å…¨åŒºåŸŸå»ºç«‹é€Ÿåº¦
            if gap_info['current'] < gap_info['target']:
                current_weights['jamming_success'] *= (1 + adjustment_strength)
                current_weights['jamming_attempt_reward'] *= (1 + adjustment_strength)
                current_weights['goal_reward'] *= (1 + adjustment_strength * 0.5)
                
        elif metric == 'reconnaissance_cooperation':
            # æé«˜ä¾¦å¯Ÿåä½œç‡
            if gap_info['current'] < gap_info['target']:
                current_weights['coordination_reward'] *= (1 + adjustment_strength)
                current_weights['approach_reward'] *= (1 + adjustment_strength * 0.5)
                
        elif metric == 'jamming_cooperation':
            # æé«˜å¹²æ‰°åä½œç‡
            if gap_info['current'] < gap_info['target']:
                current_weights['coordination_reward'] *= (1 + adjustment_strength)
                current_weights['jamming_success'] *= (1 + adjustment_strength)
                current_weights['partial_success'] *= (1 + adjustment_strength)
                
        elif metric == 'jamming_failure_rate':
            # é™ä½å¹²æ‰°å¤±æ•ˆç‡
            if gap_info['current'] > gap_info['target']:
                current_weights['jamming_attempt_reward'] *= (1 + adjustment_strength)
                current_weights['approach_reward'] *= (1 + adjustment_strength)
                current_weights['distance_penalty'] *= (1 - adjustment_strength * 0.3)
                
        elif metric == 'success_rate':
            # æé«˜æ•´ä½“æˆåŠŸç‡
            if gap_info['current'] < gap_info['target']:
                current_weights['goal_reward'] *= (1 + adjustment_strength)
                current_weights['jamming_success'] *= (1 + adjustment_strength)
                current_weights['death_penalty'] *= (1 - adjustment_strength * 0.5)
                
        elif metric == 'average_reward':
            # æé«˜å¹³å‡å¥–åŠ±
            if gap_info['current'] < gap_info['target']:
                current_weights['reward_scale'] *= (1 + adjustment_strength * 0.3)
                current_weights['max_reward'] *= (1 + adjustment_strength * 0.2)
                # å‡å°‘æƒ©ç½š
                for key in ['distance_penalty', 'energy_penalty', 'detection_penalty']:
                    if current_weights[key] < 0:
                        current_weights[key] *= (1 - adjustment_strength * 0.3)
        
        return current_weights
    
    def _smooth_weight_changes(self, old_weights, new_weights, smoothing_factor=0.7):
        """å¹³æ»‘æƒé‡å˜åŒ–ï¼Œé¿å…å‰§çƒˆè°ƒæ•´"""
        smoothed = {}
        
        for key in old_weights:
            if key in new_weights:
                # ä½¿ç”¨æŒ‡æ•°å¹³æ»‘
                smoothed[key] = old_weights[key] * smoothing_factor + new_weights[key] * (1 - smoothing_factor)
                
                # é™åˆ¶å˜åŒ–å¹…åº¦
                max_change = abs(old_weights[key]) * 0.2  # æœ€å¤§å˜åŒ–20%
                change = smoothed[key] - old_weights[key]
                if abs(change) > max_change:
                    smoothed[key] = old_weights[key] + np.sign(change) * max_change
            else:
                smoothed[key] = old_weights[key]
        
        return smoothed
    
    def _record_adjustment(self, metrics, new_weights, critical_metric):
        """è®°å½•è°ƒæ•´å†å²"""
        adjustment_record = {
            'timestamp': datetime.now().isoformat(),
            'metrics': metrics.copy(),
            'new_weights': new_weights.copy(),
            'critical_metric': critical_metric
        }
        
        self.adjustment_history.append(adjustment_record)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.adjustment_history) > 100:
            self.adjustment_history = self.adjustment_history[-100:]
    
    def get_progressive_curriculum_weights(self, episode_count, total_episodes):
        """
        è·å–æ¸è¿›å¼è¯¾ç¨‹å­¦ä¹ çš„æƒé‡
        
        Args:
            episode_count: å½“å‰å›åˆæ•°
            total_episodes: æ€»å›åˆæ•°
            
        Returns:
            dict: è¯¾ç¨‹è°ƒæ•´åçš„æƒé‡
        """
        progress = episode_count / total_episodes
        weights = self.base_weights.copy()
        
        # æ—©æœŸè®­ç»ƒï¼šæ›´å¤šæ¢ç´¢å¥–åŠ±ï¼Œè¾ƒå°‘æƒ©ç½š
        if progress < 0.3:
            weights['jamming_attempt_reward'] *= 2.0  # é¼“åŠ±å°è¯•
            weights['approach_reward'] *= 1.5
            weights['distance_penalty'] *= 0.5  # å‡å°‘æ¢ç´¢æƒ©ç½š
            weights['energy_penalty'] *= 0.5
            
        # ä¸­æœŸè®­ç»ƒï¼šå¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨
        elif progress < 0.7:
            weights['coordination_reward'] *= 1.5  # å¢åŠ åä½œå¥–åŠ±
            weights['partial_success'] *= 1.3
            
        # åæœŸè®­ç»ƒï¼šæ›´æ³¨é‡ç²¾ç¡®æ§åˆ¶å’Œé«˜è´¨é‡å®Œæˆ
        else:
            weights['goal_reward'] *= 1.5  # æ›´é‡è§†ç›®æ ‡å®Œæˆ
            weights['jamming_success'] *= 1.3
            weights['reward_scale'] *= 1.2  # æ”¾å¤§å¥–åŠ±ä¿¡å·
        
        return weights
    
    def evaluate_reward_effectiveness(self, recent_performance, window_size=50):
        """
        è¯„ä¼°å¥–åŠ±è®¾è®¡çš„æœ‰æ•ˆæ€§
        
        Args:
            recent_performance: æœ€è¿‘çš„æ€§èƒ½æ•°æ®åˆ—è¡¨
            window_size: è¯„ä¼°çª—å£å¤§å°
            
        Returns:
            dict: å¥–åŠ±æœ‰æ•ˆæ€§è¯„ä¼°ç»“æœ
        """
        if len(recent_performance) < window_size:
            return {'status': 'insufficient_data'}
        
        # è·å–æœ€è¿‘çš„æ€§èƒ½æ•°æ®
        recent_data = recent_performance[-window_size:]
        
        # è®¡ç®—è¶‹åŠ¿
        trends = {}
        for metric in self.target_metrics:
            if metric in recent_data[0]:
                values = [data[metric] for data in recent_data if metric in data]
                if len(values) >= 2:
                    trend = np.polyfit(range(len(values)), values, 1)[0]
                    trends[metric] = trend
        
        # è¯„ä¼°æ”¹è¿›é€Ÿåº¦
        improvement_rate = np.mean([abs(trend) for trend in trends.values() if not np.isnan(trend)])
        
        # è®¡ç®—ç›®æ ‡æ¥è¿‘åº¦
        if recent_data:
            latest_metrics = recent_data[-1]
            approach_scores = []
            
            for metric, target in self.target_metrics.items():
                if metric in latest_metrics:
                    current = latest_metrics[metric]
                    if metric == 'jamming_failure_rate':
                        score = max(0, 1 - abs(current - target) / target)
                    else:
                        score = max(0, 1 - abs(current - target) / target)
                    approach_scores.append(score)
            
            average_approach = np.mean(approach_scores) if approach_scores else 0
        else:
            average_approach = 0
        
        return {
            'status': 'evaluated',
            'trends': trends,
            'improvement_rate': improvement_rate,
            'target_approach_score': average_approach,
            'recommendation': self._get_adjustment_recommendation(trends, average_approach)
        }
    
    def _get_adjustment_recommendation(self, trends, approach_score):
        """è·å–è°ƒæ•´å»ºè®®"""
        if approach_score > 0.8:
            return "æ€§èƒ½æ¥è¿‘ç›®æ ‡ï¼Œç»§ç»­å½“å‰ç­–ç•¥"
        elif approach_score > 0.6:
            return "æ€§èƒ½è‰¯å¥½ï¼Œå¯å¾®è°ƒå¥–åŠ±æƒé‡"
        elif approach_score > 0.4:
            return "æ€§èƒ½ä¸€èˆ¬ï¼Œéœ€è¦è°ƒæ•´å¥–åŠ±ç­–ç•¥"
        else:
            return "æ€§èƒ½è¾ƒå·®ï¼Œéœ€è¦å¤§å¹…è°ƒæ•´å¥–åŠ±è®¾è®¡"
    
    def save_design_history(self, save_path):
        """ä¿å­˜å¥–åŠ±è®¾è®¡å†å²"""
        history_data = {
            'target_metrics': self.target_metrics,
            'base_weights': self.base_weights,
            'adjustment_history': self.adjustment_history,
            'total_adjustments': len(self.adjustment_history)
        }
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(history_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ å¥–åŠ±è®¾è®¡å†å²å·²ä¿å­˜åˆ°: {save_path}")

class RewardShapeOptimizer:
    """å¥–åŠ±å¡‘å½¢ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.shaping_functions = {
            'exponential_decay': self._exponential_decay_shaping,
            'sigmoid_shaping': self._sigmoid_shaping,
            'linear_interpolation': self._linear_interpolation_shaping,
            'threshold_based': self._threshold_based_shaping
        }
    
    def _exponential_decay_shaping(self, distance, max_distance, decay_rate=0.1):
        """æŒ‡æ•°è¡°å‡å¥–åŠ±å¡‘å½¢"""
        normalized_distance = distance / max_distance
        return np.exp(-decay_rate * normalized_distance)
    
    def _sigmoid_shaping(self, value, midpoint, steepness=1.0):
        """Sigmoidå¥–åŠ±å¡‘å½¢"""
        return 1 / (1 + np.exp(-steepness * (value - midpoint)))
    
    def _linear_interpolation_shaping(self, value, min_val, max_val):
        """çº¿æ€§æ’å€¼å¥–åŠ±å¡‘å½¢"""
        return np.clip((value - min_val) / (max_val - min_val), 0, 1)
    
    def _threshold_based_shaping(self, value, thresholds, rewards):
        """åŸºäºé˜ˆå€¼çš„å¥–åŠ±å¡‘å½¢"""
        for i, threshold in enumerate(thresholds):
            if value <= threshold:
                return rewards[i]
        return rewards[-1]
    
    def optimize_distance_reward(self, uav_position, radar_position, max_range=1000):
        """ä¼˜åŒ–è·ç¦»ç›¸å…³å¥–åŠ±"""
        distance = np.linalg.norm(uav_position - radar_position)
        
        # ä½¿ç”¨å¤šé˜¶æ®µå¥–åŠ±
        if distance < 200:  # è¿‘è·ç¦» - é«˜å¥–åŠ±
            return 1.0
        elif distance < 500:  # ä¸­è·ç¦» - ä¸­ç­‰å¥–åŠ±
            return self._linear_interpolation_shaping(distance, 200, 500) * 0.7 + 0.3
        elif distance < max_range:  # è¿œè·ç¦» - ä½å¥–åŠ±
            return self._exponential_decay_shaping(distance, max_range, 0.002)
        else:  # è¶…å‡ºèŒƒå›´ - æ— å¥–åŠ±
            return 0.0
    
    def optimize_cooperation_reward(self, uav_positions, cooperation_threshold=800):
        """ä¼˜åŒ–åä½œå¥–åŠ±"""
        if len(uav_positions) < 2:
            return 0.0
        
        cooperation_score = 0.0
        pair_count = 0
        
        for i in range(len(uav_positions)):
            for j in range(i + 1, len(uav_positions)):
                distance = np.linalg.norm(uav_positions[i] - uav_positions[j])
                
                # ç†æƒ³åä½œè·ç¦»å¥–åŠ±
                if 200 <= distance <= cooperation_threshold:
                    normalized_distance = distance / cooperation_threshold
                    score = self._sigmoid_shaping(normalized_distance, 0.5, 5.0)
                    cooperation_score += score
                
                pair_count += 1
        
        return cooperation_score / pair_count if pair_count > 0 else 0.0

if __name__ == "__main__":
    # æµ‹è¯•æ™ºèƒ½å¥–åŠ±è®¾è®¡å™¨
    designer = IntelligentRewardDesigner()
    
    # æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®
    test_metrics = {
        'reconnaissance_completion': 0.45,
        'safe_zone_time': 0.8,
        'reconnaissance_cooperation': 20.0,
        'jamming_cooperation': 15.0,
        'jamming_failure_rate': 35.0,
        'success_rate': 0.2,
        'average_reward': 400.0
    }
    
    # åˆ†ææ€§èƒ½å·®è·
    analysis = designer.analyze_performance_gap(test_metrics)
    print("æ€§èƒ½å·®è·åˆ†æ:", analysis)
    
    # æµ‹è¯•å¥–åŠ±å¡‘å½¢ä¼˜åŒ–å™¨
    shaper = RewardShapeOptimizer()
    
    # æµ‹è¯•è·ç¦»å¥–åŠ±
    distance_reward = shaper.optimize_distance_reward(
        np.array([100, 100, 0]), 
        np.array([300, 300, 0])
    )
    print(f"è·ç¦»å¥–åŠ±: {distance_reward:.3f}")
    
    # æµ‹è¯•åä½œå¥–åŠ±
    positions = [np.array([0, 0, 0]), np.array([400, 400, 0]), np.array([800, 800, 0])]
    coop_reward = shaper.optimize_cooperation_reward(positions)
    print(f"åä½œå¥–åŠ±: {coop_reward:.3f}")
    
    print("âœ… æ™ºèƒ½å¥–åŠ±è®¾è®¡å™¨æµ‹è¯•å®Œæˆ!") 