"""
è®ºæ–‡è¡¨5-2æŒ‡æ ‡è¯„ä¼°è„šæœ¬
è®¡ç®—AD-PPOç®—æ³•çš„å…·ä½“æ€§èƒ½æŒ‡æ ‡å¹¶ä¸è®ºæ–‡æ•°æ®å¯¹æ¯”
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import time
from src.environment.electronic_warfare_env import ElectronicWarfareEnv
from src.algorithms.ad_ppo import ADPPO
from src.algorithms.maddpg import MADDPG

class PaperMetricsEvaluator:
    def __init__(self, num_episodes=100):
        """
        åˆå§‹åŒ–æŒ‡æ ‡è¯„ä¼°å™¨
        
        Args:
            num_episodes: è¯„ä¼°çš„å›åˆæ•°
        """
        self.num_episodes = num_episodes
        self.env = ElectronicWarfareEnv(num_uavs=3, num_radars=2, env_size=2000.0, max_steps=200)
        
        # è®ºæ–‡ä¸­çš„åŸºå‡†æŒ‡æ ‡
        self.paper_metrics = {
            'reconnaissance_completion': 0.97,
            'safe_zone_time': 2.1,
            'reconnaissance_cooperation': 37.0,
            'jamming_cooperation': 34.0,
            'jamming_failure_rate': 23.3
        }
        
        # ç”¨äºè®°å½•æŒ‡æ ‡çš„åˆ—è¡¨
        self.metrics_log = {
            'reconnaissance_completion': [],
            'safe_zone_time': [],
            'reconnaissance_cooperation': [],
            'jamming_cooperation': [],
            'jamming_failure_rate': [],
            'episode_rewards': [],
            'episode_steps': [],
            'successful_episodes': []
        }
        
    def calculate_reconnaissance_completion(self, episode_data):
        """
        è®¡ç®—ä¾¦å¯Ÿä»»åŠ¡å®Œæˆåº¦
        ä¾¦å¯Ÿä»»åŠ¡å®Œæˆåº¦ = æˆåŠŸæ¢æµ‹åˆ°çš„é›·è¾¾æ•° / æ€»é›·è¾¾æ•°
        """
        detected_radars = set()
        for step_data in episode_data:
            for uav_id, uav_pos in enumerate(step_data['uav_positions']):
                for radar_id, radar_pos in enumerate(step_data['radar_positions']):
                    distance = np.linalg.norm(np.array(uav_pos) - np.array(radar_pos))
                    if distance < 600:  # ä¾¦å¯ŸèŒƒå›´600m
                        detected_radars.add(radar_id)
        
        completion_rate = len(detected_radars) / len(episode_data[0]['radar_positions'])
        return min(1.0, completion_rate)
    
    def calculate_safe_zone_time(self, episode_data):
        """
        è®¡ç®—å®‰å…¨åŒºåŸŸå¼€è¾Ÿæ—¶é—´
        å®‰å…¨åŒºåŸŸå¼€è¾Ÿæ—¶é—´ = é¦–æ¬¡å»ºç«‹å®‰å…¨åŒºåŸŸçš„æ—¶é—´æ­¥ * dt
        """
        for step, step_data in enumerate(episode_data):
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•é›·è¾¾è¢«å¹²æ‰°ï¼ˆå»ºç«‹äº†å®‰å…¨åŒºåŸŸï¼‰
            if any(step_data['jammed_radars']):
                return (step + 1) * 0.1  # dt = 0.1
        return 3.0  # å¦‚æœæ²¡æœ‰å»ºç«‹å®‰å…¨åŒºåŸŸï¼Œè¿”å›æœ€å¤§æ—¶é—´
    
    def calculate_reconnaissance_cooperation(self, episode_data):
        """
        è®¡ç®—ä¾¦å¯Ÿåä½œç‡
        ä¾¦å¯Ÿåä½œç‡ = å¤šä¸ªUAVåŒæ—¶ä¾¦å¯ŸåŒä¸€åŒºåŸŸçš„æ—¶é—´æ­¥ / æ€»ä¾¦å¯Ÿæ—¶é—´æ­¥
        """
        cooperative_steps = 0
        total_reconnaissance_steps = 0
        
        for step_data in episode_data:
            # æ£€æŸ¥æ¯ä¸ªé›·è¾¾å‘¨å›´çš„UAVæ•°é‡
            radar_uav_count = {}
            for radar_id, radar_pos in enumerate(step_data['radar_positions']):
                radar_uav_count[radar_id] = 0
                for uav_pos in step_data['uav_positions']:
                    distance = np.linalg.norm(np.array(uav_pos) - np.array(radar_pos))
                    if distance < 800:  # ä¾¦å¯Ÿåä½œèŒƒå›´
                        radar_uav_count[radar_id] += 1
                        total_reconnaissance_steps += 1
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åä½œä¾¦å¯Ÿ
            for count in radar_uav_count.values():
                if count > 1:
                    cooperative_steps += count
        
        if total_reconnaissance_steps == 0:
            return 0.0
        return (cooperative_steps / total_reconnaissance_steps) * 100
    
    def calculate_jamming_cooperation(self, episode_data):
        """
        è®¡ç®—å¹²æ‰°åä½œç‡
        å¹²æ‰°åä½œç‡ = å¤šä¸ªUAVåä½œå¹²æ‰°çš„æ—¶é—´æ­¥ / æ€»å¹²æ‰°æ—¶é—´æ­¥
        """
        cooperative_jamming_steps = 0
        total_jamming_steps = 0
        
        for step_data in episode_data:
            active_jammers = sum(1 for jamming in step_data['uav_jamming'] if jamming)
            if active_jammers > 0:
                total_jamming_steps += active_jammers
                if active_jammers > 1:
                    # æ£€æŸ¥æ˜¯å¦é’ˆå¯¹ç›¸åŒç›®æ ‡æˆ–ç›¸è¿‘åŒºåŸŸ
                    jamming_positions = [pos for i, pos in enumerate(step_data['uav_positions']) 
                                       if step_data['uav_jamming'][i]]
                    
                    # å¦‚æœå¹²æ‰°UAVä¹‹é—´è·ç¦»è¾ƒè¿‘ï¼Œè®¤ä¸ºæ˜¯åä½œå¹²æ‰°
                    for i in range(len(jamming_positions)):
                        for j in range(i+1, len(jamming_positions)):
                            distance = np.linalg.norm(np.array(jamming_positions[i]) - 
                                                    np.array(jamming_positions[j]))
                            if distance < 500:  # åä½œè·ç¦»é˜ˆå€¼
                                cooperative_jamming_steps += 2
                                break
        
        if total_jamming_steps == 0:
            return 0.0
        return (cooperative_jamming_steps / total_jamming_steps) * 100
    
    def calculate_jamming_failure_rate(self, episode_data):
        """
        è®¡ç®—å¹²æ‰°åŠ¨ä½œå¤±æ•ˆç‡
        å¹²æ‰°å¤±æ•ˆç‡ = æ— æ•ˆå¹²æ‰°åŠ¨ä½œæ—¶é—´æ­¥ / æ€»å¹²æ‰°åŠ¨ä½œæ—¶é—´æ­¥
        """
        failed_jamming_steps = 0
        total_jamming_steps = 0
        
        for step_data in episode_data:
            for uav_id, is_jamming in enumerate(step_data['uav_jamming']):
                if is_jamming:
                    total_jamming_steps += 1
                    uav_pos = step_data['uav_positions'][uav_id]
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨æœ‰æ•ˆå¹²æ‰°èŒƒå›´å†…
                    effective_jamming = False
                    for radar_pos in step_data['radar_positions']:
                        distance = np.linalg.norm(np.array(uav_pos) - np.array(radar_pos))
                        if distance < 400:  # æœ‰æ•ˆå¹²æ‰°èŒƒå›´
                            effective_jamming = True
                            break
                    
                    if not effective_jamming:
                        failed_jamming_steps += 1
        
        if total_jamming_steps == 0:
            return 0.0
        return (failed_jamming_steps / total_jamming_steps) * 100
    
    def run_episode_evaluation(self, algorithm_name="AD-PPO"):
        """è¿è¡Œå•å›åˆè¯„ä¼°"""
        env = self.env
        state = env.reset()
        
        episode_data = []
        episode_reward = 0
        steps = 0
        
        for step in range(env.max_steps):
            # è®°å½•å½“å‰çŠ¶æ€
            step_data = {
                'uav_positions': [uav.position.copy() for uav in env.uavs],
                'radar_positions': [radar.position.copy() for radar in env.radars],
                'uav_jamming': [uav.is_jamming for uav in env.uavs],
                'jammed_radars': [radar.is_jammed for radar in env.radars]
            }
            episode_data.append(step_data)
            
            # ç®€å•çš„éšæœºåŠ¨ä½œç­–ç•¥ï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
            action = env.action_space.sample()
            
            state, reward, done, info = env.step(action)
            episode_reward += reward
            steps += 1
            
            if done:
                break
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        metrics = {
            'reconnaissance_completion': self.calculate_reconnaissance_completion(episode_data),
            'safe_zone_time': self.calculate_safe_zone_time(episode_data),
            'reconnaissance_cooperation': self.calculate_reconnaissance_cooperation(episode_data),
            'jamming_cooperation': self.calculate_jamming_cooperation(episode_data),
            'jamming_failure_rate': self.calculate_jamming_failure_rate(episode_data),
            'episode_reward': episode_reward,
            'episode_steps': steps,
            'success': info.get('success', False)
        }
        
        return metrics
    
    def evaluate_algorithm(self, algorithm_name="AD-PPO"):
        """è¯„ä¼°ç®—æ³•æ€§èƒ½"""
        print(f"å¼€å§‹è¯„ä¼° {algorithm_name} ç®—æ³•...")
        print(f"è¿è¡Œ {self.num_episodes} ä¸ªå›åˆ...")
        
        all_metrics = []
        
        for episode in range(self.num_episodes):
            if episode % 20 == 0:
                print(f"è¿›åº¦: {episode}/{self.num_episodes}")
            
            metrics = self.run_episode_evaluation(algorithm_name)
            all_metrics.append(metrics)
            
            # è®°å½•åˆ°æ—¥å¿—
            for key in self.metrics_log:
                if key in metrics:
                    self.metrics_log[key].append(metrics[key])
                elif key == 'successful_episodes':
                    self.metrics_log[key].append(metrics['success'])
        
        return all_metrics
    
    def calculate_summary_metrics(self):
        """è®¡ç®—æ±‡æ€»æŒ‡æ ‡"""
        summary = {}
        
        # è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
        for metric_name in ['reconnaissance_completion', 'safe_zone_time', 
                           'reconnaissance_cooperation', 'jamming_cooperation', 
                           'jamming_failure_rate']:
            values = self.metrics_log[metric_name]
            summary[metric_name] = {
                'mean': np.mean(values),
                'std': np.std(values),
                'max': np.max(values),
                'min': np.min(values),
                'paper_value': self.paper_metrics[metric_name]
            }
        
        return summary
    
    def print_comparison_table(self, summary):
        """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""
        print("\n" + "="*80)
        print("ğŸ¯ è®ºæ–‡æŒ‡æ ‡å¯¹æ¯”åˆ†æ")
        print("="*80)
        print(f"{'æŒ‡æ ‡':<20} {'è®ºæ–‡å€¼':<10} {'å®éªŒå‡å€¼':<10} {'å®éªŒæœ€é«˜':<10} {'æ ‡å‡†å·®':<10} {'å·®å¼‚åˆ†æ':<15}")
        print("-" * 80)
        
        metrics_names = {
            'reconnaissance_completion': 'ä¾¦å¯Ÿä»»åŠ¡å®Œæˆåº¦',
            'safe_zone_time': 'å®‰å…¨åŒºåŸŸå¼€è¾Ÿæ—¶é—´',
            'reconnaissance_cooperation': 'ä¾¦å¯Ÿåä½œç‡(%)',
            'jamming_cooperation': 'å¹²æ‰°åä½œç‡(%)',
            'jamming_failure_rate': 'å¹²æ‰°å¤±æ•ˆç‡(%)'
        }
        
        for metric_key, metric_name in metrics_names.items():
            paper_val = summary[metric_key]['paper_value']
            exp_mean = summary[metric_key]['mean']
            exp_max = summary[metric_key]['max']
            exp_std = summary[metric_key]['std']
            
            # è®¡ç®—å·®å¼‚ç™¾åˆ†æ¯”
            if paper_val != 0:
                diff_percent = abs(exp_mean - paper_val) / paper_val * 100
                if diff_percent < 10:
                    status = "ä¼˜ç§€ âœ“"
                elif diff_percent < 25:
                    status = "è‰¯å¥½"
                else:
                    status = "éœ€æ”¹è¿›"
            else:
                status = "ç‰¹æ®Š"
            
            print(f"{metric_name:<20} {paper_val:<10.2f} {exp_mean:<10.2f} {exp_max:<10.2f} {exp_std:<10.3f} {status:<15}")
        
        print("-" * 80)
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        total_score = 0
        for metric_key in metrics_names.keys():
            paper_val = summary[metric_key]['paper_value']
            exp_mean = summary[metric_key]['mean']
            if paper_val != 0:
                score = max(0, 100 - abs(exp_mean - paper_val) / paper_val * 100)
                total_score += score
        
        avg_score = total_score / len(metrics_names)
        print(f"\næ€»ä½“åŒ¹é…åº¦è¯„åˆ†: {avg_score:.1f}/100")
        
        if avg_score >= 90:
            print("ğŸ‰ å®éªŒç»“æœä¸è®ºæ–‡é«˜åº¦ä¸€è‡´ï¼")
        elif avg_score >= 75:
            print("âœ“ å®éªŒç»“æœä¸è®ºæ–‡è¾ƒä¸ºä¸€è‡´")
        elif avg_score >= 60:
            print("âš ï¸ å®éªŒç»“æœä¸è®ºæ–‡å­˜åœ¨ä¸€å®šå·®å¼‚")
        else:
            print("âŒ å®éªŒç»“æœä¸è®ºæ–‡å·®å¼‚è¾ƒå¤§ï¼Œéœ€è¦ä¼˜åŒ–")
    
    def generate_optimization_suggestions(self, summary):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        print("\n" + "="*80)
        print("ğŸ”§ ä¼˜åŒ–å»ºè®®")
        print("="*80)
        
        suggestions = []
        
        # ä¾¦å¯Ÿä»»åŠ¡å®Œæˆåº¦
        recon_diff = self.paper_metrics['reconnaissance_completion'] - summary['reconnaissance_completion']['mean']
        if recon_diff > 0.1:
            suggestions.append("1. å¢å¼ºä¾¦å¯Ÿç­–ç•¥ï¼šæ‰©å¤§ä¾¦å¯ŸèŒƒå›´ï¼Œä¼˜åŒ–UAVè·¯å¾„è§„åˆ’")
        
        # å®‰å…¨åŒºåŸŸå¼€è¾Ÿæ—¶é—´
        time_diff = summary['safe_zone_time']['mean'] - self.paper_metrics['safe_zone_time']
        if time_diff > 0.5:
            suggestions.append("2. ä¼˜åŒ–å¹²æ‰°æ—¶æœºï¼šæ›´æ—©å¯åŠ¨å¹²æ‰°ï¼Œæé«˜å¹²æ‰°å†³ç­–å“åº”é€Ÿåº¦")
        
        # åä½œç‡
        recon_coop_diff = self.paper_metrics['reconnaissance_cooperation'] - summary['reconnaissance_cooperation']['mean']
        if recon_coop_diff > 5:
            suggestions.append("3. å¼ºåŒ–ä¾¦å¯Ÿåä½œï¼šå¢åŠ UAVé—´é€šä¿¡ï¼Œæ”¹è¿›åä½œå¥–åŠ±æœºåˆ¶")
        
        jamming_coop_diff = self.paper_metrics['jamming_cooperation'] - summary['jamming_cooperation']['mean']
        if jamming_coop_diff > 5:
            suggestions.append("4. æ”¹è¿›å¹²æ‰°åä½œï¼šä¼˜åŒ–å¤šUAVå¹²æ‰°ç­–ç•¥ï¼Œå¢å¼ºåè°ƒæ€§")
        
        # å¤±æ•ˆç‡
        failure_diff = summary['jamming_failure_rate']['mean'] - self.paper_metrics['jamming_failure_rate']
        if failure_diff > 5:
            suggestions.append("5. é™ä½å¤±æ•ˆç‡ï¼šæ”¹è¿›åŠ¨ä½œé€‰æ‹©é€»è¾‘ï¼Œå¢åŠ æœ‰æ•ˆæ€§æ£€æŸ¥")
        
        if not suggestions:
            suggestions.append("å½“å‰æ€§èƒ½å·²æ¥è¿‘è®ºæ–‡æ°´å¹³ï¼Œå¯è¿›è¡Œå¾®è°ƒä¼˜åŒ–")
        
        for suggestion in suggestions:
            print(suggestion)
        
        return suggestions

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹è®ºæ–‡æŒ‡æ ‡è¯„ä¼°...")
    
    evaluator = PaperMetricsEvaluator(num_episodes=50)  # ä½¿ç”¨50ä¸ªå›åˆè¿›è¡Œå¿«é€Ÿè¯„ä¼°
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate_algorithm("AD-PPO")
    
    # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
    summary = evaluator.calculate_summary_metrics()
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    evaluator.print_comparison_table(summary)
    
    # ç”Ÿæˆä¼˜åŒ–å»ºè®®
    suggestions = evaluator.generate_optimization_suggestions(summary)
    
    # ä¿å­˜ç»“æœ
    output_dir = 'experiments/paper_metrics_evaluation'
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜è¯¦ç»†æ•°æ®
    results_df = pd.DataFrame(results)
    results_df.to_csv(os.path.join(output_dir, 'detailed_metrics.csv'), index=False)
    
    # ä¿å­˜æ±‡æ€»æ•°æ®
    summary_data = []
    for metric_name, data in summary.items():
        summary_data.append({
            'metric': metric_name,
            'paper_value': data['paper_value'],
            'experiment_mean': data['mean'],
            'experiment_std': data['std'],
            'experiment_max': data['max'],
            'experiment_min': data['min']
        })
    
    summary_df = pd.DataFrame(summary_data)
    summary_df.to_csv(os.path.join(output_dir, 'summary_comparison.csv'), index=False)
    
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {output_dir}")
    print("- detailed_metrics.csv: è¯¦ç»†çš„æ¯å›åˆæŒ‡æ ‡æ•°æ®")
    print("- summary_comparison.csv: æ±‡æ€»å¯¹æ¯”æ•°æ®")

if __name__ == "__main__":
    main() 