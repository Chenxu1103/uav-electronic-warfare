#!/usr/bin/env python3
"""
ä¼˜åŒ–è®­ç»ƒç³»ç»Ÿ - é€æ­¥æ¥è¿‘è®ºæ–‡ç†æƒ³æ•°æ®

æœ¬ç³»ç»ŸåŒ…å«ä»¥ä¸‹ä¼˜åŒ–:
1. æ”¹è¿›çš„ç®—æ³•å®ç°
2. ä¼˜åŒ–çš„ç¯å¢ƒè®¾è®¡
3. æ™ºèƒ½è®­ç»ƒç­–ç•¥
4. è‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜
5. æ¸è¿›å¼è®­ç»ƒæµç¨‹
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from datetime import datetime
import json
import pandas as pd
import matplotlib.pyplot as plt
from collections import deque
import random

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from src.environment.electronic_warfare_env import ElectronicWarfareEnv
from src.algorithms.ad_ppo import ADPPO
from src.algorithms.maddpg import MADDPG

class OptimizedTrainingSystem:
    """ä¼˜åŒ–çš„è®­ç»ƒç³»ç»Ÿ"""
    
    def __init__(self, config=None):
        self.config = self._get_default_config()
        if config:
            self.config.update(config)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.env = self._create_optimized_environment()
        self.performance_tracker = PerformanceTracker()
        self.hyperparameter_optimizer = HyperparameterOptimizer()
        self.curriculum_trainer = CurriculumTrainer()
        
    def _get_default_config(self):
        """è·å–é»˜è®¤é…ç½®"""
        return {
            # ç¯å¢ƒå‚æ•°
            'num_uavs': 3,
            'num_radars': 2,
            'env_size': 2000.0,
            'max_steps': 200,
            
            # è®­ç»ƒå‚æ•°
            'total_episodes': 2000,
            'evaluation_interval': 50,
            'save_interval': 100,
            'batch_size': 64,
            
            # ä¼˜åŒ–å‚æ•°
            'target_performance': {
                'reconnaissance_completion': 0.90,
                'safe_zone_time': 2.0,
                'reconnaissance_cooperation': 35.0,
                'jamming_cooperation': 30.0,
                'jamming_failure_rate': 25.0
            },
            
            # è‡ªé€‚åº”å‚æ•°
            'adaptive_learning': True,
            'curriculum_learning': True,
            'auto_tuning': True
        }
    
    def _create_optimized_environment(self):
        """åˆ›å»ºä¼˜åŒ–çš„ç¯å¢ƒ"""
        env = ElectronicWarfareEnv(
            num_uavs=self.config['num_uavs'],
            num_radars=self.config['num_radars'],
            env_size=self.config['env_size'],
            max_steps=self.config['max_steps']
        )
        
        # ä¼˜åŒ–å¥–åŠ±æƒé‡
        env.reward_weights.update({
            'jamming_success': 100.0,           # å¢åŠ å¹²æ‰°æˆåŠŸå¥–åŠ±
            'partial_success': 50.0,            # éƒ¨åˆ†æˆåŠŸå¥–åŠ±
            'distance_penalty': -0.00005,       # å‡å°è·ç¦»æƒ©ç½š
            'energy_penalty': -0.005,           # å‡å°èƒ½é‡æƒ©ç½š
            'detection_penalty': -0.1,          # å‡å°æ£€æµ‹æƒ©ç½š
            'death_penalty': -1.0,              # å‡å°æ­»äº¡æƒ©ç½š
            'goal_reward': 1000.0,              # å¢åŠ ç›®æ ‡å¥–åŠ±
            'coordination_reward': 50.0,        # å¢åŠ åè°ƒå¥–åŠ±
            'stealth_reward': 1.0,              # å¢åŠ éšèº«å¥–åŠ±
            'approach_reward': 15.0,            # å¢åŠ æ¥è¿‘å¥–åŠ±
            'jamming_attempt_reward': 8.0,      # å¢åŠ å°è¯•å¹²æ‰°å¥–åŠ±
            'reward_scale': 0.8,                # å¢åŠ å¥–åŠ±ç¼©æ”¾
            'min_reward': -10.0,                # è°ƒæ•´æœ€å°å¥–åŠ±
            'max_reward': 150.0,                # å¢åŠ æœ€å¤§å¥–åŠ±
        })
        
        return env
    
    def train_optimized_adppo(self):
        """è®­ç»ƒä¼˜åŒ–çš„AD-PPOç®—æ³•"""
        print("ğŸš€ å¼€å§‹ä¼˜åŒ–AD-PPOè®­ç»ƒ...")
        
        # åˆ›å»ºä¼˜åŒ–çš„AD-PPOæ™ºèƒ½ä½“
        agent = self._create_optimized_adppo()
        
        # æ¸è¿›å¼è®­ç»ƒ
        results = self.curriculum_trainer.progressive_training(
            agent=agent,
            env=self.env,
            config=self.config,
            performance_tracker=self.performance_tracker
        )
        
        return agent, results
    
    def train_optimized_maddpg(self):
        """è®­ç»ƒä¼˜åŒ–çš„MADDPGç®—æ³•"""
        print("ğŸš€ å¼€å§‹ä¼˜åŒ–MADDPGè®­ç»ƒ...")
        
        # åˆ›å»ºä¼˜åŒ–çš„MADDPGæ™ºèƒ½ä½“
        agent = self._create_optimized_maddpg()
        
        # æ¸è¿›å¼è®­ç»ƒ
        results = self.curriculum_trainer.progressive_training(
            agent=agent,
            env=self.env,
            config=self.config,
            performance_tracker=self.performance_tracker,
            algorithm_type='maddpg'
        )
        
        return agent, results
    
    def _create_optimized_adppo(self):
        """åˆ›å»ºä¼˜åŒ–çš„AD-PPOæ™ºèƒ½ä½“"""
        state_dim = self.env.observation_space.shape[0]
        action_dim = self.env.action_space.shape[0]
        
        # ä½¿ç”¨ä¼˜åŒ–çš„è¶…å‚æ•°
        optimized_params = self.hyperparameter_optimizer.get_optimal_adppo_params()
        
        agent = ADPPO(
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_dim=optimized_params['hidden_dim'],
            lr=optimized_params['learning_rate'],
            gamma=optimized_params['gamma'],
            gae_lambda=optimized_params['gae_lambda'],
            clip_param=optimized_params['clip_param'],
            value_loss_coef=optimized_params['value_loss_coef'],
            entropy_coef=optimized_params['entropy_coef'],
            max_grad_norm=optimized_params['max_grad_norm'],
            device='cpu'
        )
        
        return agent
    
    def _create_optimized_maddpg(self):
        """åˆ›å»ºä¼˜åŒ–çš„MADDPGæ™ºèƒ½ä½“"""
        state_dim = self.env.observation_space.shape[0] // self.env.num_uavs
        action_dim = self.env.action_space.shape[0] // self.env.num_uavs
        
        # ä½¿ç”¨ä¼˜åŒ–çš„è¶…å‚æ•°
        optimized_params = self.hyperparameter_optimizer.get_optimal_maddpg_params()
        
        agent = MADDPG(
            n_agents=self.env.num_uavs,
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_dim=optimized_params['hidden_dim'],
            lr_actor=optimized_params['lr_actor'],
            lr_critic=optimized_params['lr_critic'],
            gamma=optimized_params['gamma'],
            tau=optimized_params['tau'],
            batch_size=optimized_params['batch_size'],
            buffer_size=int(optimized_params['buffer_size'])
        )
        
        return agent

class PerformanceTracker:
    """æ€§èƒ½è·Ÿè¸ªå™¨"""
    
    def __init__(self, window_size=50):
        self.window_size = window_size
        self.metrics_history = {
            'rewards': deque(maxlen=window_size),
            'success_rates': deque(maxlen=window_size),
            'jamming_ratios': deque(maxlen=window_size),
            'cooperation_rates': deque(maxlen=window_size),
            'completion_rates': deque(maxlen=window_size)
        }
        
    def update(self, metrics):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        for key, value in metrics.items():
            if key in self.metrics_history:
                self.metrics_history[key].append(value)
    
    def get_trend(self, metric_name):
        """è·å–æŒ‡æ ‡è¶‹åŠ¿"""
        if metric_name not in self.metrics_history:
            return 0
        
        values = list(self.metrics_history[metric_name])
        if len(values) < 2:
            return 0
        
        # è®¡ç®—è¶‹åŠ¿æ–œç‡
        x = np.arange(len(values))
        y = np.array(values)
        slope = np.polyfit(x, y, 1)[0]
        return slope
    
    def get_current_performance(self):
        """è·å–å½“å‰æ€§èƒ½"""
        performance = {}
        for metric, values in self.metrics_history.items():
            if len(values) > 0:
                performance[metric] = {
                    'current': values[-1],
                    'average': np.mean(values),
                    'trend': self.get_trend(metric)
                }
        return performance

class HyperparameterOptimizer:
    """è¶…å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.adppo_search_space = {
            'learning_rate': [1e-4, 3e-4, 5e-4, 1e-3],
            'hidden_dim': [128, 256, 512],
            'gamma': [0.95, 0.99, 0.995],
            'gae_lambda': [0.9, 0.95, 0.98],
            'clip_param': [0.1, 0.2, 0.3],
            'value_loss_coef': [0.25, 0.5, 1.0],
            'entropy_coef': [0.005, 0.01, 0.02],
            'max_grad_norm': [0.3, 0.5, 1.0]
        }
        
        self.maddpg_search_space = {
            'lr_actor': [1e-4, 3e-4, 5e-4],
            'lr_critic': [3e-4, 5e-4, 1e-3],
            'hidden_dim': [128, 256, 512],
            'gamma': [0.95, 0.99, 0.995],
            'tau': [0.005, 0.01, 0.02],
            'batch_size': [32, 64, 128],
            'buffer_size': [5e5, 1e6, 2e6]
        }
        
        self.best_params = {
            'adppo': None,
            'maddpg': None
        }
        
    def get_optimal_adppo_params(self):
        """è·å–ä¼˜åŒ–çš„AD-PPOå‚æ•°"""
        if self.best_params['adppo'] is None:
            # è¿”å›ç»è¿‡è°ƒä¼˜çš„é»˜è®¤å‚æ•°
            self.best_params['adppo'] = {
                'learning_rate': 3e-4,
                'hidden_dim': 256,
                'gamma': 0.99,
                'gae_lambda': 0.95,
                'clip_param': 0.2,
                'value_loss_coef': 0.5,
                'entropy_coef': 0.01,
                'max_grad_norm': 0.5
            }
        return self.best_params['adppo']
    
    def get_optimal_maddpg_params(self):
        """è·å–ä¼˜åŒ–çš„MADDPGå‚æ•°"""
        if self.best_params['maddpg'] is None:
            # è¿”å›ç»è¿‡è°ƒä¼˜çš„é»˜è®¤å‚æ•°
            self.best_params['maddpg'] = {
                'lr_actor': 3e-4,
                'lr_critic': 5e-4,
                'hidden_dim': 256,
                'gamma': 0.99,
                'tau': 0.01,
                'batch_size': 64,
                'buffer_size': 1e6
            }
        return self.best_params['maddpg']
    
    def bayesian_optimization(self, algorithm_type, objective_func, n_trials=20):
        """è´å¶æ–¯ä¼˜åŒ–è¶…å‚æ•°"""
        search_space = self.adppo_search_space if algorithm_type == 'adppo' else self.maddpg_search_space
        
        best_score = -float('inf')
        best_params = None
        
        for trial in range(n_trials):
            # éšæœºé‡‡æ ·å‚æ•°ï¼ˆç®€åŒ–ç‰ˆè´å¶æ–¯ä¼˜åŒ–ï¼‰
            params = {}
            for key, values in search_space.items():
                params[key] = random.choice(values)
            
            # è¯„ä¼°å‚æ•°
            score = objective_func(params)
            
            if score > best_score:
                best_score = score
                best_params = params
                
            print(f"Trial {trial+1}/{n_trials}: Score = {score:.4f}")
        
        self.best_params[algorithm_type] = best_params
        return best_params, best_score

class CurriculumTrainer:
    """è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.difficulty_levels = [
            {'num_radars': 1, 'env_size': 1000, 'max_steps': 150},  # ç®€å•
            {'num_radars': 2, 'env_size': 1500, 'max_steps': 175},  # ä¸­ç­‰
            {'num_radars': 2, 'env_size': 2000, 'max_steps': 200},  # å›°éš¾
            {'num_radars': 3, 'env_size': 2000, 'max_steps': 200},  # ä¸“å®¶
        ]
        
    def progressive_training(self, agent, env, config, performance_tracker, algorithm_type='adppo'):
        """æ¸è¿›å¼è®­ç»ƒ"""
        print("ğŸ“š å¼€å§‹è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ...")
        
        all_results = []
        total_episodes = 0
        
        for level, difficulty in enumerate(self.difficulty_levels):
            print(f"\nğŸ¯ è®­ç»ƒéš¾åº¦ç­‰çº§ {level+1}/4: {difficulty}")
            
            # è°ƒæ•´ç¯å¢ƒéš¾åº¦
            self._adjust_environment_difficulty(env, difficulty)
            
            # è®¡ç®—è¯¥éš¾åº¦çº§åˆ«çš„è®­ç»ƒå›åˆæ•°
            episodes_for_level = config['total_episodes'] // len(self.difficulty_levels)
            
            # è®­ç»ƒè¯¥éš¾åº¦çº§åˆ«
            level_results = self._train_level(
                agent=agent,
                env=env,
                episodes=episodes_for_level,
                level=level,
                algorithm_type=algorithm_type,
                performance_tracker=performance_tracker
            )
            
            all_results.extend(level_results)
            total_episodes += episodes_for_level
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¯¥çº§åˆ«çš„é€šè¿‡æ ‡å‡†
            if self._check_level_completion(level_results):
                print(f"âœ… å®Œæˆéš¾åº¦ç­‰çº§ {level+1}")
            else:
                print(f"âš ï¸ éš¾åº¦ç­‰çº§ {level+1} éœ€è¦æ›´å¤šè®­ç»ƒ")
        
        print(f"ğŸ‰ è¯¾ç¨‹å­¦ä¹ å®Œæˆ! æ€»è®­ç»ƒå›åˆ: {total_episodes}")
        return all_results
    
    def _adjust_environment_difficulty(self, env, difficulty):
        """è°ƒæ•´ç¯å¢ƒéš¾åº¦"""
        # é‡æ–°åˆå§‹åŒ–ç¯å¢ƒå‚æ•°
        env.num_radars = difficulty['num_radars']
        env.env_size = difficulty['env_size']
        env.max_steps = difficulty['max_steps']
        
        # é‡æ–°åˆå§‹åŒ–é›·è¾¾
        env.radars = []
        for i in range(env.num_radars):
            from src.models.radar_model import Radar
            position = np.random.uniform(
                [-env.env_size/4, -env.env_size/4, 0],
                [env.env_size/4, env.env_size/4, 100]
            )
            radar = Radar(position=position)
            env.radars.append(radar)
    
    def _train_level(self, agent, env, episodes, level, algorithm_type, performance_tracker):
        """è®­ç»ƒç‰¹å®šéš¾åº¦çº§åˆ«"""
        level_results = []
        
        for episode in range(episodes):
            # è®­ç»ƒä¸€ä¸ªå›åˆ
            episode_result = self._train_episode(agent, env, algorithm_type)
            level_results.append(episode_result)
            
            # æ›´æ–°æ€§èƒ½è·Ÿè¸ª
            performance_tracker.update({
                'rewards': episode_result['reward'],
                'success_rates': episode_result['success'],
                'jamming_ratios': episode_result['jamming_ratio']
            })
            
            # æ‰“å°è¿›åº¦
            if episode % 20 == 0:
                avg_reward = np.mean([r['reward'] for r in level_results[-20:]])
                print(f"  çº§åˆ« {level+1} - å›åˆ {episode}/{episodes}, å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        
        return level_results
    
    def _train_episode(self, agent, env, algorithm_type):
        """è®­ç»ƒå•ä¸ªå›åˆ"""
        state = env.reset()
        total_reward = 0
        done = False
        step = 0
        
        while not done and step < env.max_steps:
            if algorithm_type == 'adppo':
                action, log_prob, value = agent.select_action(state)
                next_state, reward, done, info = env.step(action)
                
                # å­˜å‚¨ç»éªŒ
                agent.buffer.add(
                    state=state,
                    action=action,
                    reward=reward,
                    next_state=next_state,
                    done=done,
                    log_prob=log_prob,
                    value=value
                )
                
            else:  # maddpg
                state_dim = state.shape[0] // env.num_uavs
                agent_states = []
                for i in range(env.num_uavs):
                    agent_states.append(state[i*state_dim:(i+1)*state_dim])
                agent_states = np.array(agent_states)
                
                actions = agent.select_action(agent_states)
                combined_action = np.concatenate(actions)
                next_state, reward, done, info = env.step(combined_action)
                
                # å‡†å¤‡MADDPGçš„ç»éªŒ
                agent_next_states = []
                agent_rewards = []
                for i in range(env.num_uavs):
                    agent_next_states.append(next_state[i*state_dim:(i+1)*state_dim])
                    agent_rewards.append(reward / env.num_uavs)
                
                agent_next_states = np.array(agent_next_states)
                agent_rewards = np.array(agent_rewards)
                
                # å­˜å‚¨ç»éªŒ
                agent.replay_buffer.add(agent_states, actions, agent_next_states, agent_rewards, done)
                
                # æ›´æ–°ç­–ç•¥
                if agent.replay_buffer.size > 64:
                    agent.update()
            
            state = next_state
            total_reward += reward
            step += 1
        
        # AD-PPOçš„å›åˆç»“æŸå¤„ç†
        if algorithm_type == 'adppo' and len(agent.buffer.states) > 0:
            _, _, last_value = agent.select_action(state)
            agent.buffer.compute_returns_and_advantages(last_value, agent.gamma, agent.gae_lambda)
            rollout = agent.buffer.get()
            agent.update(rollout)
            agent.buffer.clear()
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        jammed_count = sum(1 for radar in env.radars if radar.is_jammed)
        jamming_ratio = jammed_count / len(env.radars)
        success = jamming_ratio >= 0.5
        
        return {
            'reward': total_reward,
            'success': success,
            'jamming_ratio': jamming_ratio,
            'steps': step
        }
    
    def _check_level_completion(self, level_results):
        """æ£€æŸ¥çº§åˆ«å®Œæˆæƒ…å†µ"""
        if len(level_results) < 20:
            return False
        
        # æ£€æŸ¥æœ€è¿‘20ä¸ªå›åˆçš„è¡¨ç°
        recent_results = level_results[-20:]
        avg_reward = np.mean([r['reward'] for r in recent_results])
        success_rate = np.mean([r['success'] for r in recent_results])
        
        # è®¾å®šé€šè¿‡æ ‡å‡†
        return avg_reward > 300 and success_rate > 0.3

class AdaptiveEnvironmentOptimizer:
    """è‡ªé€‚åº”ç¯å¢ƒä¼˜åŒ–å™¨"""
    
    def __init__(self, env):
        self.env = env
        self.performance_history = []
        
    def optimize_rewards(self, performance_metrics):
        """æ ¹æ®æ€§èƒ½æŒ‡æ ‡ä¼˜åŒ–å¥–åŠ±å‡½æ•°"""
        # å¦‚æœå¹²æ‰°æˆåŠŸç‡å¤ªä½ï¼Œå¢åŠ å¹²æ‰°å¥–åŠ±
        if performance_metrics.get('jamming_ratio', 0) < 0.3:
            self.env.reward_weights['jamming_success'] *= 1.1
            self.env.reward_weights['jamming_attempt_reward'] *= 1.05
        
        # å¦‚æœåä½œç‡å¤ªä½ï¼Œå¢åŠ åä½œå¥–åŠ±
        if performance_metrics.get('cooperation_rate', 0) < 0.2:
            self.env.reward_weights['coordination_reward'] *= 1.1
        
        # å¦‚æœå®Œæˆåº¦å¤ªä½ï¼Œå‡å°‘æƒ©ç½š
        if performance_metrics.get('completion_rate', 0) < 0.5:
            self.env.reward_weights['distance_penalty'] *= 0.9
            self.env.reward_weights['energy_penalty'] *= 0.9
        
        print(f"ğŸ”§ ç¯å¢ƒå¥–åŠ±æƒé‡å·²è‡ªé€‚åº”è°ƒæ•´")

def main():
    """ä¸»å‡½æ•° - è¿è¡Œä¼˜åŒ–è®­ç»ƒç³»ç»Ÿ"""
    print("ğŸ¯ å¯åŠ¨ä¼˜åŒ–è®­ç»ƒç³»ç»Ÿ...")
    
    # åˆ›å»ºä¼˜åŒ–è®­ç»ƒç³»ç»Ÿ
    training_system = OptimizedTrainingSystem()
    
    # åˆ›å»ºç»“æœç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = f"experiments/optimized_training/{timestamp}"
    os.makedirs(results_dir, exist_ok=True)
    
    # è®­ç»ƒAD-PPO
    print("\nğŸš€ å¼€å§‹ä¼˜åŒ–AD-PPOè®­ç»ƒ...")
    adppo_agent, adppo_results = training_system.train_optimized_adppo()
    
    # è®­ç»ƒMADDPG
    print("\nğŸš€ å¼€å§‹ä¼˜åŒ–MADDPGè®­ç»ƒ...")
    maddpg_agent, maddpg_results = training_system.train_optimized_maddpg()
    
    # ä¿å­˜æ¨¡å‹
    adppo_agent.save(os.path.join(results_dir, "optimized_adppo_model.pt"))
    torch.save(maddpg_agent.state_dict(), os.path.join(results_dir, "optimized_maddpg_model.pt"))
    
    # è¯„ä¼°æœ€ç»ˆæ€§èƒ½
    print("\nğŸ“Š è¯„ä¼°æœ€ç»ˆæ€§èƒ½...")
    final_evaluation = evaluate_optimized_performance(
        adppo_agent, maddpg_agent, training_system.env, results_dir
    )
    
    print(f"âœ… ä¼˜åŒ–è®­ç»ƒå®Œæˆ! ç»“æœä¿å­˜åœ¨: {results_dir}")
    return final_evaluation

def evaluate_optimized_performance(adppo_agent, maddpg_agent, env, save_dir):
    """è¯„ä¼°ä¼˜åŒ–åçš„æ€§èƒ½"""
    from enhanced_performance_comparison import collect_detailed_episode_data, create_table_5_2_comparison
    
    print("æ­£åœ¨è¯„ä¼°ä¼˜åŒ–åçš„æ€§èƒ½...")
    
    # æ”¶é›†è¯¦ç»†æ€§èƒ½æ•°æ®
    adppo_results = collect_detailed_episode_data(adppo_agent, env, 'adppo', 50)
    maddpg_results = collect_detailed_episode_data(maddpg_agent, env, 'maddpg', 50)
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    comparison_df = create_table_5_2_comparison(adppo_results, maddpg_results, save_dir)
    
    # æ‰“å°æ”¹è¿›æƒ…å†µ
    print("\nğŸ“ˆ ä¼˜åŒ–æ•ˆæœæ€»ç»“:")
    print("=" * 60)
    print(f"{'æŒ‡æ ‡':<25} {'AD-PPO':<15} {'MADDPG':<15} {'ç›®æ ‡å€¼':<15}")
    print("-" * 60)
    print(f"{'ä¾¦å¯Ÿä»»åŠ¡å®Œæˆåº¦':<25} {adppo_results['reconnaissance_completion']:<15.3f} {maddpg_results['reconnaissance_completion']:<15.3f} {'0.90':<15}")
    print(f"{'å®‰å…¨åŒºåŸŸå¼€è¾Ÿæ—¶é—´':<25} {adppo_results['safe_zone_development_time']:<15.1f} {maddpg_results['safe_zone_development_time']:<15.1f} {'2.0':<15}")
    print(f"{'ä¾¦å¯Ÿåä½œç‡(%)':<25} {adppo_results['reconnaissance_cooperation_rate']:<15.1f} {maddpg_results['reconnaissance_cooperation_rate']:<15.1f} {'35.0':<15}")
    print(f"{'å¹²æ‰°åä½œç‡(%)':<25} {adppo_results['jamming_cooperation_rate']:<15.1f} {maddpg_results['jamming_cooperation_rate']:<15.1f} {'30.0':<15}")
    print(f"{'å¹²æ‰°åŠ¨ä½œå¤±æ•ˆç‡(%)':<25} {adppo_results['jamming_failure_rate']:<15.1f} {maddpg_results['jamming_failure_rate']:<15.1f} {'25.0':<15}")
    
    return {
        'adppo': adppo_results,
        'maddpg': maddpg_results,
        'comparison': comparison_df
    }

if __name__ == "__main__":
    main() 